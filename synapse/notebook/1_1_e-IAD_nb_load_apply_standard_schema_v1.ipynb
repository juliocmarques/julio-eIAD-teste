{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Change source file to standard schema\n",
        "Reads the file with uploaded data and applies the standard naming schema to the columns. The output is dataset with new columns names without any change to the data values.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 20\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "statistics_path = ''\n",
        "data_encoding = ''\n",
        "data_separator = ''\n",
        "invoice_uncompressed_path = ''\n",
        "invoice_schema_applied_path = ''\n",
        "output_container_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import datetime\n",
        "import time\n",
        "import csv\n",
        "import pyodbc\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, isnan, when, count, current_timestamp\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, FloatType"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'statistics_path': statistics_path,\n",
        "    'data_encoding': data_encoding,\n",
        "    'data_separator': data_separator,\n",
        "    'invoice_uncompressed_path': invoice_uncompressed_path,\n",
        "    'invoice_schema_applied_path': invoice_schema_applied_path,\n",
        "    'output_container_path': output_container_path,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# serverless SQL config\n",
        "database = 'eiad'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SyanpseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Create new row to append to DataFrame\n",
        "row = [[f'{batch_id}', 'Starting Data Loading']]\n",
        "columns = ['batch_id', 'status']\n",
        "new_batch_status_df = spark.createDataFrame(row, columns)\n",
        "new_batch_status_df = new_batch_status_df.withColumn(\"date_submitted\", current_timestamp())\n",
        "new_batch_status_df = new_batch_status_df.withColumn(\"update_time_stamp\", current_timestamp())\n",
        "\n",
        "# update the batch status table with new row to use in Power BI and webapp\n",
        "new_batch_status_df.write.mode(\"append\").parquet(f'{output_container_path}/batch_status')\n",
        "logger.info(f'Create new batch status entry of \"Started Data Loading\" for batch id: {batch_id}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def generate_schema_string(dataframe):\n",
        "    schema_string = \"\"\n",
        "    for name in dataframe.schema.fieldNames():\n",
        "        schema_string += \"[\" + name + \"] \"\n",
        "        datatype = str(dataframe.schema[name].dataType.simpleString())\n",
        "        if datatype == 'double': datatype = 'float'\n",
        "        if datatype == 'string': datatype = 'nvarchar(MAX)'\n",
        "        if datatype == 'timestamp': datatype = 'datetime2(7)'\n",
        "        schema_string += datatype + \", \"\n",
        "    return schema_string[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "table_name = 'batch_status'\n",
        "schema_string = generate_schema_string(new_batch_status_df)\n",
        "drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "location = 'batch_status'\n",
        "df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "    with conn.cursor() as cursor:\n",
        "        try:\n",
        "            cursor.execute(drop_table_command)\n",
        "        except:\n",
        "            pass\n",
        "        cursor.execute(df_sql_command)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def deep_ls(path: str, max_depth=1):\n",
        "    \"\"\"\n",
        "    List all files and folders in specified path and\n",
        "    subfolders within maximum recursion depth.\n",
        "    \"\"\"\n",
        "\n",
        "    # List all files in path\n",
        "    li = mssparkutils.fs.ls(path)\n",
        "\n",
        "    # Return all files\n",
        "    for x in li:\n",
        "        if x.size != 0:\n",
        "            yield x\n",
        "\n",
        "    # If the max_depth has not been reached, start\n",
        "    # listing files and folders in subdirectories\n",
        "    if max_depth > 1:\n",
        "        for x in li:\n",
        "            if x.size != 0:\n",
        "                continue\n",
        "            for y in deep_ls(x.path, max_depth - 1):\n",
        "                yield y\n",
        "\n",
        "    # If max_depth has been reached,\n",
        "    # return the folders\n",
        "    else:\n",
        "        for x in li:\n",
        "            if x.size == 0:\n",
        "                yield x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def applySchema(fileName, fullFilePath) :\n",
        "    df = spark.read.csv(fullFilePath, sep=data_separator,inferSchema=True, header=True)\n",
        "\n",
        "    col_names = ['issuer_type', 'issuer_id','activity_issuer','receiver_type', 'receiver_id', \n",
        "    'document_type', 'document_id','issued_date','sales_terms','credit_term',\n",
        "    'currency','exchange_rate_r','payment_method1','payment_method2','payment_method3',\n",
        "    'payment_method4','payment_method5','payment_method99','total_taxable_services','total_non_taxable_services',\n",
        "    'total_taxable_goods','total_non_taxable_goods','total_taxable','total_non_taxable','total_sales',\n",
        "    'total_discounts','total_voucher','total_tax']\n",
        "\n",
        "    df = df.select(*col_names)\n",
        "\n",
        "    # Change data types\n",
        "    df = df.withColumn('issuer_type' , df['issuer_type'].cast(StringType()))\n",
        "    df = df.withColumn('issuer_id' , df['issuer_id'].cast(StringType()))\n",
        "    df = df.withColumn('activity_issuer' , df['activity_issuer'].cast(StringType()))\n",
        "    df = df.withColumn('receiver_type' , df['receiver_type'].cast(StringType()))\n",
        "    df = df.withColumn('receiver_id' , df['receiver_id'].cast(StringType()))\n",
        "    df = df.withColumn('document_type' , df['document_type'].cast(StringType()))\n",
        "    df = df.withColumn('document_id' , df['document_id'].cast(StringType()))\n",
        "    df = df.withColumn('issued_date' , df['issued_date'].cast(TimestampType()))\n",
        "    df = df.withColumn('sales_terms' , df['sales_terms'].cast(StringType()))\n",
        "    df = df.withColumn('credit_term' , df['credit_term'].cast(IntegerType()))\n",
        "    df = df.withColumn('currency' , df['currency'].cast(StringType()))\n",
        "    df = df.withColumn('exchange_rate_r' , df['exchange_rate_r'].cast(FloatType()))\n",
        "    df = df.withColumn('payment_method1' , df['payment_method1'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method2' , df['payment_method2'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method3' , df['payment_method3'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method4' , df['payment_method4'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method5' , df['payment_method5'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method5' , df['payment_method5'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method99' , df['payment_method99'].cast(StringType()))\n",
        "    df = df.withColumn('total_taxable_services' , df['total_taxable_services'].cast(FloatType()))\n",
        "    df = df.withColumn('total_non_taxable_services' , df['total_non_taxable_services'].cast(FloatType()))\n",
        "    df = df.withColumn('total_taxable_goods' , df['total_taxable_goods'].cast(FloatType()))\n",
        "    df = df.withColumn('total_non_taxable_goods' , df['total_non_taxable_goods'].cast(FloatType()))\n",
        "    df = df.withColumn('total_taxable' , df['total_taxable'].cast(FloatType()))\n",
        "    df = df.withColumn('total_non_taxable' , df['total_non_taxable'].cast(FloatType()))\n",
        "    df = df.withColumn('total_sales' , df['total_sales'].cast(FloatType()))\n",
        "    df = df.withColumn('total_discounts' , df['total_discounts'].cast(FloatType()))\n",
        "    df = df.withColumn('total_voucher' , df['total_voucher'].cast(FloatType()))\n",
        "    df = df.withColumn('total_tax' , df['total_tax'].cast(FloatType()))\n",
        "     \n",
        "    df.write.mode(\"overwrite\").csv(f'{invoice_schema_applied_path}/{fileName}', header=True, sep=data_separator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "file_names = deep_ls(invoice_uncompressed_path,20)\n",
        "for filename in file_names:  \n",
        "    with tracer.span('Applying schema on invoice file'):\n",
        "        logger.info(f'Processing file: {filename.name}')\n",
        "        applySchema(filename.name.replace('.CSV',''), filename.path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
