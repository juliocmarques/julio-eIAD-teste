{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Calculate uploaded data statistics\n",
        "Calculates statistics of the uploaded data\n",
        "\n",
        "\n",
        "- Total number of sales invoices reported \n",
        "\n",
        "- Total number of credit notes reported \n",
        " \n",
        "- Total number of debit notes reported \n",
        " \n",
        "- Number of issuer_ids in the entired period \n",
        " \n",
        "- Number of identified receiver_ids in the period \n",
        " \n",
        "- Number of ISICs (activity) reported in the period "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 40\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "invoice_schema_applied_path = ''\n",
        "statistics_path = ''\n",
        "data_separator = ''\n",
        "data_encoding = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pandas as pd\n",
        "import datetime\n",
        "import csv\n",
        "import time\n",
        "import pyodbc\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType,DoubleType\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, isnan, when, count "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'statistics_path': statistics_path,\n",
        "    'data_encoding': data_encoding,\n",
        "    'data_separator': data_separator,\n",
        "    'invoice_schema_applied_path': invoice_schema_applied_path,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "schema_uploaded_data_statistics = StructType([ \n",
        "    StructField(\"filename\",StringType(),True), \n",
        "    StructField(\"month\",StringType(),True), \n",
        "    StructField(\"num_invoices\",DoubleType(),True),\n",
        "    StructField(\"num_deb_notes\",DoubleType(),True),\n",
        "    StructField(\"num_credit_notes\",DoubleType(),True),\n",
        "    StructField(\"num_sellers\",DoubleType(),True),\n",
        "    StructField(\"num_buyers\",DoubleType(),True),\n",
        "    StructField(\"number_isics\",DoubleType(),True),\n",
        "    StructField(\"number_records\",DoubleType(),True),\n",
        "    StructField(\"num_purchase_docs\",DoubleType(),True),\n",
        "    StructField(\"num_export_docs\",DoubleType(),True)\n",
        "  ])\n",
        "\n",
        "schema_total_documents = StructType([\n",
        "    StructField('document_type', StringType(), True),\n",
        "    StructField('total_documents', IntegerType(), True)\n",
        "])\n",
        "\n",
        "schema_total_unique = StructType([\n",
        "    StructField('unique_issuer_id', IntegerType(), True),\n",
        "    StructField('unique_receiver_id', IntegerType(), True),\n",
        "    StructField('unique_activity_id', IntegerType(), True)\n",
        "])\n",
        "\n",
        "\n",
        "unique_issuer_id_dataset = pd.DataFrame()\n",
        "unique_receiver_id_dataset = pd.DataFrame()\n",
        "unique_activity_id_dataset = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "df_uploaded_data_statistics = spark.createDataFrame(emptyRDD, schema=schema_uploaded_data_statistics)\n",
        "\n",
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "df_statistics_uniques = spark.createDataFrame(emptyRDD, schema=schema_total_unique)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def deep_ls(path: str, max_depth=1):\n",
        "    \"\"\"\n",
        "    List all files and folders in specified path and\n",
        "    subfolders within maximum recursion depth.\n",
        "    \"\"\"\n",
        "\n",
        "    # List all files in path\n",
        "    li = mssparkutils.fs.ls(path)\n",
        "\n",
        "    # Return all files\n",
        "    for x in li:\n",
        "        if x.size != 0:\n",
        "            yield x\n",
        "\n",
        "    # If the max_depth has not been reached, start\n",
        "    # listing files and folders in subdirectories\n",
        "    if max_depth > 1:\n",
        "        for x in li:\n",
        "            if x.size != 0:\n",
        "                continue\n",
        "            for y in deep_ls(x.path, max_depth - 1):\n",
        "                yield y\n",
        "\n",
        "    # If max_depth has been reached,\n",
        "    # return the folders\n",
        "    else:\n",
        "        for x in li:\n",
        "            if x.size == 0:\n",
        "                yield x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def getStatistics(df_uploaded_data_statistics, fileName, fullFilePath) :\n",
        "\n",
        "    global unique_issuer_id_dataset \n",
        "    global unique_receiver_id_dataset\n",
        "    global unique_activity_id_dataset\n",
        "\n",
        "    df = spark.read.csv(fullFilePath, sep=data_separator,inferSchema=True, header=True)\n",
        "\n",
        "\n",
        "    df_counts = pd.DataFrame(df.groupBy('document_type').count().collect())\n",
        "\n",
        "    df_total_document_by_type = spark.createDataFrame(df_counts, schema_total_documents)\n",
        "\n",
        "    #invoices\n",
        "    total_by_type = df_total_document_by_type.filter(df_total_document_by_type.document_type == 'I').collect()\n",
        "    if len(total_by_type) > 0 :\n",
        "        total_number_sale_invoices = total_by_type[0][1]\n",
        "    else :\n",
        "        total_number_sale_invoices = 0\n",
        "    \n",
        "    #debit notes\n",
        "    total_by_type = df_total_document_by_type.filter(df_total_document_by_type.document_type == 'D').collect()\n",
        "    if len(total_by_type) > 0 :\n",
        "        total_number_debit_notes = total_by_type[0][1]\n",
        "    else :\n",
        "        total_number_debit_notes = 0\n",
        "\n",
        "    #credit notes\n",
        "    total_by_type = df_total_document_by_type.filter(df_total_document_by_type.document_type == 'C').collect()\n",
        "    if len(total_by_type) > 0 :\n",
        "        total_number_credit_notes = total_by_type[0][1]\n",
        "    else :\n",
        "        total_number_credit_notes = 0        \n",
        "\n",
        "    #purchase\n",
        "    total_by_type = df_total_document_by_type.filter(df_total_document_by_type.document_type == 'P').collect()\n",
        "    if len(total_by_type) > 0 :\n",
        "        total_number_purchase = total_by_type[0][1]\n",
        "    else :\n",
        "        total_number_purchase = 0  \n",
        "\n",
        "    #export invoice\n",
        "    total_by_type = df_total_document_by_type.filter(df_total_document_by_type.document_type == 'X').collect()\n",
        "    if len(total_by_type) > 0 :\n",
        "        total_number_export = total_by_type[0][1]\n",
        "    else :\n",
        "        total_number_export = 0          \n",
        "\n",
        "    total_issuer_ids = df.select('issuer_id').distinct().count()\n",
        "    total_receiver_ids = df.select('receiver_id').distinct().count()\n",
        "    total_ISICs = df.select('activity_issuer').distinct().count()\n",
        "\n",
        "    total_rows_in_dataset = df.count()\n",
        "\n",
        "\n",
        "    file_name_splits = (fileName.split(\"-\"))\n",
        "    month_number = (file_name_splits[2])\n",
        "    year_of_analysis = (file_name_splits[3])\n",
        "    month_of_analysis = month_number + ' - ' + year_of_analysis\n",
        "\n",
        "    new_row = [[fileName,month_of_analysis, total_number_sale_invoices, total_number_debit_notes, \n",
        "    total_number_credit_notes, total_issuer_ids , total_receiver_ids, total_ISICs, \n",
        "    total_rows_in_dataset, total_number_purchase, total_number_export]]\n",
        "    \n",
        "    unknown_df = spark.createDataFrame(new_row)\n",
        "    df_uploaded_data_statistics = df_uploaded_data_statistics.union(unknown_df)\n",
        "\n",
        "    unique_issuer_id_in_file = pd.DataFrame(df.select('issuer_id').distinct().collect())\n",
        "    unique_receiver_id_in_file = pd.DataFrame(df.select('receiver_id').distinct().collect())\n",
        "    unique_activity_id_in_file = pd.DataFrame(df.select('activity_issuer').distinct().collect())\n",
        "\n",
        "    unique_issuer_id_dataset = unique_issuer_id_dataset.append(unique_issuer_id_in_file, ignore_index=True)\n",
        "    unique_receiver_id_dataset = unique_receiver_id_dataset.append(unique_receiver_id_in_file, ignore_index=True)\n",
        "    unique_activity_id_dataset = unique_activity_id_dataset.append(unique_activity_id_in_file, ignore_index=True)\n",
        "\n",
        "\n",
        "    return df_uploaded_data_statistics    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "file_names = mssparkutils.fs.ls(invoice_schema_applied_path)\n",
        "for filename in file_names:\n",
        "    logger.info(f'Calculating data statistics for file: {filename.name}')  \n",
        "    with tracer.span('Calculating data statistics for invoice file'):\n",
        "        df_uploaded_data_statistics = getStatistics(df_uploaded_data_statistics, filename.name, filename.path)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "total_unique_issuer_id_dataset = unique_issuer_id_dataset[0].nunique()\n",
        "total_unique_receiver_id_dataset = unique_receiver_id_dataset[0].nunique()\n",
        "total_unique_activity_id_dataset = unique_activity_id_dataset[0].nunique()\n",
        "\n",
        "new_row = [[total_unique_issuer_id_dataset, total_unique_receiver_id_dataset, total_unique_activity_id_dataset]]\n",
        "unknown_df_time = spark.createDataFrame(new_row)\n",
        "\n",
        "df_statistics_uniques = df_statistics_uniques.union(unknown_df_time)\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write out the statistics\n",
        "if the notebook receives the \"run_time_stamp\" parameter from Synapse Ingest the parameter is used to create a folder to store the statistics, if no parameter is received we write the statistics to the default output folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Writing data statistics to ADLS'):\n",
        "    #writeout statistics\n",
        "    df_uploaded_data_statistics.repartition(1).write.mode(\"overwrite\").parquet(f'{statistics_path}load_stats')\n",
        "    df_statistics_uniques.repartition(1).write.mode(\"overwrite\").parquet(f'{statistics_path}load_stats_unique_receiver_issuer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write out SQL table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# serverless SQL config\n",
        "database = 'eiad'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SyanpseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def generate_schema_string(dataframe):\n",
        "    schema_string = \"\"\n",
        "    for name in dataframe.schema.fieldNames():\n",
        "        schema_string += \"[\" + name + \"] \"\n",
        "        datatype = str(dataframe.schema[name].dataType.simpleString())\n",
        "        if datatype == 'double': datatype = 'float'\n",
        "        if datatype == 'string': datatype = 'nvarchar(MAX)'\n",
        "        if datatype == 'timestamp': datatype = 'datetime2(7)'\n",
        "        schema_string += datatype + \", \"\n",
        "    return schema_string[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for invoice load statistics'):\n",
        "    table_name = statistics_path.split('/')[3] + '_' + statistics_path.split('/')[2].split('@')[0] + '_' + statistics_path.split('/')[4] + '_' + 'load_stats'\n",
        "    schema_string = generate_schema_string(df_uploaded_data_statistics)\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "    location = \"/\".join([i for idx, i in enumerate(statistics_path.split('/')) if idx > 2]) + 'load_stats'\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            try:\n",
        "                cursor.execute(drop_table_command)\n",
        "            except:\n",
        "                pass\n",
        "            cursor.execute(df_sql_command)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for invoice unique statistics'):\n",
        "    table_name = statistics_path.split('/')[3] + '_' + statistics_path.split('/')[2].split('@')[0] + '_' + statistics_path.split('/')[4] + '_' + 'load_stats_unique_receiver_issuer'\n",
        "    schema_string = generate_schema_string(df_statistics_uniques)\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "    location = \"/\".join([i for idx, i in enumerate(statistics_path.split('/')) if idx > 2]) + 'load_stats_unique_receiver_issuer'\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            try:\n",
        "                cursor.execute(drop_table_command)\n",
        "            except:\n",
        "                pass\n",
        "            cursor.execute(df_sql_command)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
