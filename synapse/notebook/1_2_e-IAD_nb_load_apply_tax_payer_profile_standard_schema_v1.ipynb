{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Change source tax payer profile to standard schema\n",
        "Reads the file with the tax payer profiles and applies the standard naming schema to the columns. The output is dataset with new columns names without any change to the data values.\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 20\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "taxpayer_profile_uncompressed_path = ''\n",
        "taxpayer_profile_schema_applied_path = ''\n",
        "statistics_path = ''\n",
        "data_separator = ''\n",
        "data_encoding = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import datetime\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "from calendar import monthrange\n",
        "import time\n",
        "import pyodbc\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, isnan, when, count\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, FloatType"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'statistics_path': statistics_path,\n",
        "    'data_encoding': data_encoding,\n",
        "    'data_separator': data_separator,\n",
        "    'taxpayer_profile_uncompressed_path': taxpayer_profile_uncompressed_path,\n",
        "    'taxpayer_profile_schema_applied_path': taxpayer_profile_schema_applied_path,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def deep_ls(path: str, max_depth=1):\n",
        "    \"\"\"\n",
        "    List all files and folders in specified path and\n",
        "    subfolders within maximum recursion depth.\n",
        "    \"\"\"\n",
        "\n",
        "    # List all files in path\n",
        "    li = mssparkutils.fs.ls(path)\n",
        "\n",
        "    # Return all files\n",
        "    for x in li:\n",
        "        if x.size != 0:\n",
        "            yield x\n",
        "\n",
        "    # If the max_depth has not been reached, start\n",
        "    # listing files and folders in subdirectories\n",
        "    if max_depth > 1:\n",
        "        for x in li:\n",
        "            if x.size != 0:\n",
        "                continue\n",
        "            for y in deep_ls(x.path, max_depth - 1):\n",
        "                yield y\n",
        "\n",
        "    # If max_depth has been reached,\n",
        "    # return the folders\n",
        "    else:\n",
        "        for x in li:\n",
        "            if x.size == 0:\n",
        "                yield x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def applySchema(fileName, fullFilePath) :\n",
        "    df = spark.read.csv(fullFilePath, sep=data_separator,inferSchema=True, header=True)\n",
        "\n",
        "    col_names = ['taxpayer_id', 'taxpayer_type','fiscal_condition',\n",
        "    'regime_name', 'taxpayer_size',\n",
        "    'main_activity', 'sec1_activity','sec2_activity','employees_number',\n",
        "    'legal_reg_date', 'tax_reg_date','e_inv_enroll_date','reported_assets',\n",
        "    'total_capital','social_capital', 'total_assets',\n",
        "    'total_fixed_assets','total_liabilities','gross_income',\n",
        "    'net_income','total_vat_sales','credited_einvoicing_value',\n",
        "    'state','municipality','city']\n",
        "\n",
        "    df = df.select(*col_names)\n",
        "\n",
        "    # Change data types\n",
        "    df = df.withColumn('taxpayer_id' , df['taxpayer_id'].cast(StringType()))\n",
        "    df = df.withColumn('taxpayer_type' , df['taxpayer_type'].cast(StringType()))\n",
        "    df = df.withColumn('fiscal_condition' , df['fiscal_condition'].cast(StringType()))\n",
        "    df = df.withColumn('regime_name' , df['regime_name'].cast(StringType()))\n",
        "    df = df.withColumn('taxpayer_size' , df['taxpayer_size'].cast(StringType()))\n",
        "    df = df.withColumn('main_activity' , df['main_activity'].cast(StringType()))\n",
        "    df = df.withColumn('sec1_activity' , df['sec1_activity'].cast(StringType()))\n",
        "    df = df.withColumn('sec2_activity' , df['sec2_activity'].cast(StringType()))\n",
        "    df = df.withColumn('employees_number' , df['employees_number'].cast(IntegerType()))\n",
        "    df = df.withColumn('legal_reg_date' , df['legal_reg_date'].cast(DateType()))\n",
        "    df = df.withColumn('tax_reg_date' , df['tax_reg_date'].cast(DateType()))\n",
        "    df = df.withColumn('e_inv_enroll_date' , df['e_inv_enroll_date'].cast(DateType()))\n",
        "    df = df.withColumn('reported_assets' , df['reported_assets'].cast(IntegerType()))\n",
        "    df = df.withColumn('total_capital' , df['total_capital'].cast(FloatType()))\n",
        "    df = df.withColumn('social_capital' , df['social_capital'].cast(FloatType()))\n",
        "    df = df.withColumn('total_assets' , df['total_assets'].cast(FloatType()))\n",
        "    df = df.withColumn('total_fixed_assets' , df['total_fixed_assets'].cast(FloatType()))\n",
        "    df = df.withColumn('total_liabilities' , df['total_liabilities'].cast(FloatType()))\n",
        "    df = df.withColumn('gross_income' , df['gross_income'].cast(FloatType()))\n",
        "    df = df.withColumn('net_income' , df['net_income'].cast(FloatType()))\n",
        "    df = df.withColumn('total_vat_sales' , df['total_vat_sales'].cast(FloatType()))\n",
        "    df = df.withColumn('credited_einvoicing_value' , df['credited_einvoicing_value'].cast(FloatType()))\n",
        "    df = df.withColumn('state' , df['state'].cast(StringType()))\n",
        "    df = df.withColumn('municipality' , df['municipality'].cast(StringType()))\n",
        "    df = df.withColumn('city' , df['city'].cast(StringType()))\n",
        "    \n",
        "    df.write.mode(\"overwrite\").option(\"encoding\", \"UTF-8\").csv(f'{taxpayer_profile_schema_applied_path}tax_payer_profile', header=True, sep=data_separator)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "file_names = deep_ls(taxpayer_profile_uncompressed_path,20)\n",
        "for filename in file_names:  \n",
        "    with tracer.span('Applying schema on taxpayer profile file'):\n",
        "        logger.info(f'Processing file: {filename.name}')\n",
        "        applySchema(filename.name.replace('.CSV',''), filename.path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
