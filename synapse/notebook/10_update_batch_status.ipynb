{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "tags": []
      },
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 20\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\r\n",
        "output_container_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyodbc\r\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, isnan, when, count, current_timestamp, to_date, lit\r\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\r\n",
        "import logging\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\r\n",
        "from opencensus.trace import config_integration\r\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\r\n",
        "from opencensus.trace.tracer import Tracer\r\n",
        "\r\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\r\n",
        "config_integration.trace_integrations(['logging'])\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\r\n",
        "logger.setLevel(logging.INFO)\r\n",
        "\r\n",
        "tracer = Tracer(\r\n",
        "    exporter=AzureExporter(\r\n",
        "        connection_string=instrumentation_connection_string\r\n",
        "    ),\r\n",
        "    sampler=AlwaysOnSampler()\r\n",
        ")\r\n",
        "\r\n",
        "# Spool parameters\r\n",
        "run_time_parameters = {'custom_dimensions': {\r\n",
        "    'batch_id': batch_id,\r\n",
        "    'output_container_path': output_container_path,\r\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "  \r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# serverless SQL config\r\n",
        "database = 'eiad'\r\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\r\n",
        "\r\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\r\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\r\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SyanpseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Update Status Table\r\n",
        "def get_recent_status(batch_id, driver, serverless_sql_endpoint, database, sql_user_name, sql_user_pwd):\r\n",
        "    query = f\"\"\"\r\n",
        "        SELECT TOP (1) \r\n",
        "        [batch_id], [date_submitted]\r\n",
        "        FROM [dbo].[batch_status] \r\n",
        "        WHERE [batch_id] = '{batch_id}' \r\n",
        "        ORDER BY [update_time_stamp] DESC;\r\n",
        "    \"\"\"\r\n",
        "    with pyodbc.connect(f'DRIVER={driver};SERVER=tcp:{serverless_sql_endpoint};PORT=1433;DATABASE={database};UID={sql_user_name};PWD={sql_user_pwd}') as conn:\r\n",
        "        with conn.cursor() as cursor:\r\n",
        "            cursor.execute(query)\r\n",
        "            ret_batch_id, date_submitted = cursor.fetchone()\r\n",
        "            return date_submitted\r\n",
        "\r\n",
        "with tracer.span('Updating batch status to \"Anomaly Detection Complete\"'):\r\n",
        "    date_submitted = get_recent_status(batch_id, driver, serverless_sql_endpoint, database, sql_user_name, sql_user_pwd)\r\n",
        "    row = [[f'{batch_id}', 'Anomaly Detection Complete']]\r\n",
        "    columns = ['batch_id', 'status']\r\n",
        "    new_batch_status_df = spark.createDataFrame(row, columns)\r\n",
        "    new_batch_status_df.show()\r\n",
        "    new_batch_status_df = new_batch_status_df.withColumn(\"date_submitted\", lit(date_submitted))\r\n",
        "    new_batch_status_df = new_batch_status_df.withColumn(\"update_time_stamp\", current_timestamp())\r\n",
        "    new_batch_status_df.write.mode(\"append\").parquet(f'{output_container_path}/batch_status')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
