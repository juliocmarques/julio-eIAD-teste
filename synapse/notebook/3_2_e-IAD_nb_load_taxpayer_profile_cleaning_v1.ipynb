{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      },
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 20\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\r\n",
        "taxpayer_profile_schema_applied_path = ''\r\n",
        "taxpayer_profile_cleaned_path = ''\r\n",
        "statistics_path = ''\r\n",
        "data_separator = ''\r\n",
        "data_encoding = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import datetime\r\n",
        "import csv\r\n",
        "import pandas as pd\r\n",
        "from datetime import date\r\n",
        "from calendar import monthrange\r\n",
        "import time\r\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, isnan, when, count\r\n",
        "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, DoubleType, FloatType"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\r\n",
        "import logging\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\r\n",
        "from opencensus.trace import config_integration\r\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\r\n",
        "from opencensus.trace.tracer import Tracer\r\n",
        "\r\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\r\n",
        "config_integration.trace_integrations(['logging'])\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\r\n",
        "logger.setLevel(logging.INFO)\r\n",
        "\r\n",
        "tracer = Tracer(\r\n",
        "    exporter=AzureExporter(\r\n",
        "        connection_string=instrumentation_connection_string\r\n",
        "    ),\r\n",
        "    sampler=AlwaysOnSampler()\r\n",
        ")\r\n",
        "\r\n",
        "# Spool parameters\r\n",
        "run_time_parameters = {'custom_dimensions': {\r\n",
        "    'batch_id': batch_id,\r\n",
        "    'statistics_path': statistics_path,\r\n",
        "    'data_encoding': data_encoding,\r\n",
        "    'data_separator': data_separator,\r\n",
        "    'taxpayer_profile_schema_applied_path': taxpayer_profile_schema_applied_path,\r\n",
        "    'taxpayer_profile_cleaned_path': taxpayer_profile_cleaned_path,\r\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "  \r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def deep_ls(path: str, max_depth=1):\r\n",
        "    \"\"\"\r\n",
        "    List all files and folders in specified path and\r\n",
        "    subfolders within maximum recursion depth.\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    # List all files in path\r\n",
        "    li = mssparkutils.fs.ls(path)\r\n",
        "\r\n",
        "    # Return all files\r\n",
        "    for x in li:\r\n",
        "        if x.size != 0:\r\n",
        "            yield x\r\n",
        "\r\n",
        "    # If the max_depth has not been reached, start\r\n",
        "    # listing files and folders in subdirectories\r\n",
        "    if max_depth > 1:\r\n",
        "        for x in li:\r\n",
        "            if x.size != 0:\r\n",
        "                continue\r\n",
        "            for y in deep_ls(x.path, max_depth - 1):\r\n",
        "                yield y\r\n",
        "\r\n",
        "    # If max_depth has been reached,\r\n",
        "    # return the folders\r\n",
        "    else:\r\n",
        "        for x in li:\r\n",
        "            if x.size == 0:\r\n",
        "                yield x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Tax payer profile cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Loading schema applied taxpayer profile file'):\r\n",
        "    fullFilePath = taxpayer_profile_schema_applied_path + 'tax_payer_profile'\r\n",
        "    df = spark.read.csv(fullFilePath, sep=data_separator,inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "col_names = ['taxpayer_id', 'taxpayer_type','fiscal_condition',\r\n",
        "    'regime_name', 'taxpayer_size',\r\n",
        "    'main_activity', 'sec1_activity','sec2_activity','employees_number',\r\n",
        "    'legal_reg_date', 'tax_reg_date','e_inv_enroll_date','reported_assets',\r\n",
        "    'total_capital','social_capital', 'total_assets',\r\n",
        "    'total_fixed_assets','total_liabilities','gross_income',\r\n",
        "    'net_income','total_vat_sales','credited_einvoicing_value',\r\n",
        "    'state','municipality','city']\r\n",
        "\r\n",
        "stringtype_col_names = ['taxpayer_id','taxpayer_type', 'fiscal_condition',\r\n",
        "'regime_name','taxpayer_size','main_activity', \r\n",
        "'sec1_activity','sec2_activity','state','municipality','city']\r\n",
        "\r\n",
        "datetype_col_names = ['legal_reg_date', 'tax_reg_date','e_inv_enroll_date']\r\n",
        "\r\n",
        "numerictype_col_names = ['employees_number',\r\n",
        "    'total_capital','social_capital','reported_assets','total_assets',\r\n",
        "    'total_fixed_assets','total_liabilities','gross_income',\r\n",
        "    'net_income','total_vat_sales','credited_einvoicing_value']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#change all columns to string type in order to clean it before type conversion\r\n",
        "df = df.select([col('`{}`'.format(c)).cast(StringType()).alias(c) for c in col_names])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Invalid values handling\r\n",
        "\r\n",
        "In the input dataset several numeric, date and string columns have invalid values like #N/A, -, null, NULL we clean this data here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# replace invalid values in columns, string columns will be set to null initially and in the next process we will replace with the proper value specific for each column\r\n",
        "for colname in stringtype_col_names:\r\n",
        "    df = df.withColumn(colname, \r\n",
        "       when((col(colname) == \"\")  | (col(colname) == \"#N/A\") | (col(colname) == \"-\") | (col(colname) == \"null\") | (col(colname) == \"NULL\"), None) \r\n",
        "          .otherwise(col(colname))) \r\n",
        "\r\n",
        "\r\n",
        "for colname in numerictype_col_names:\r\n",
        "    df = df.withColumn(colname, \r\n",
        "       when((col(colname) == \"\")  | (col(colname) == \"#N/A\") | (col(colname) == \"-\") | (col(colname) == \"null\") | (col(colname) == \"NULL\"), 0) \r\n",
        "          .otherwise(col(colname))) \r\n",
        "\r\n",
        "\r\n",
        "for colname in datetype_col_names:\r\n",
        "    df = df.withColumn(colname, \r\n",
        "       when((col(colname) == \"\")  | (col(colname) == \"#N/A\") | (col(colname) == \"-\") | (col(colname) == \"null\") | (col(colname) == \"NULL\"), \"31-12-50\") \r\n",
        "          .otherwise(col(colname))) "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Get All column names and it's types\r\n",
        "from pyspark.sql.functions import col,isnan, when, count\r\n",
        "\r\n",
        "for col in df.dtypes:\r\n",
        "    logger.info(f'{col[0]} , {col[1]}')\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Find Count of Null, None, NaN of All DataFrame Columns\r\n",
        "from pyspark.sql.functions import col,isnan, when, count\r\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\r\n",
        "   ).show(vertical=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Apply the cleansing rules for each column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#taxpayer_type\tStringType\tReplace Nulls with \"no_taxpayer_type\". Replace any value with three or more characters with \"no_taxpayer_type\"\r\n",
        "df = df.na.fill(\"no_taxpayer_type\",[\"taxpayer_type\"])\r\n",
        "\r\n",
        "#fiscal_condition\tStringType\tReplace Nulls with \"no_fiscal_condition\". Replace any value with three or more characters with \"no_fiscal_condition\"\r\n",
        "df = df.na.fill(\"no_fiscal_condition\",[\"fiscal_condition\"])\r\n",
        "\r\n",
        "#regime_name\tStringType\tReplace Nulls with \"no_regime_name\".\r\n",
        "df = df.na.fill(\"no_regime_name\",[\"regime_name\"])\r\n",
        "\r\n",
        "#taxpayer_size\tStringType\tReplace Nulls with \"no_taxpayer_size\". Convert long names to the two character codes. Convert any local language name to the English equivalent in two digit codes.\r\n",
        "df = df.na.fill(\"no_taxpayer_size\",[\"taxpayer_size\"])\r\n",
        "\r\n",
        "#main_activity\tStringType\tReplace Nulls with code \"999999\". Replace any value < 3 characters and > 8 characters with code \"999999\". Replace any value between 4 to 8 characters that is not a numeric string characters with code \"999999\".\r\n",
        "df = df.na.fill(\"999999\",[\"main_activity\"])\r\n",
        "\r\n",
        "#sec1_activity\tStringType\tReplace Nulls with code \"999999\". Replace any value < 3 characters and > 8 characters with code \"999999\". Replace any value between 4 to 8 characters that is not a numeric string characters with code \"999999\".\r\n",
        "df = df.na.fill(\"999999\",[\"sec1_activity\"])\r\n",
        "\r\n",
        "#sec2_activity\tStringType\tReplace Nulls with code \"999999\". Replace any value < 3 characters and > 8 characters with code \"999999\". Replace any value between 4 to 8 characters that is not a numeric string characters with code \"999999\".\r\n",
        "df = df.na.fill(\"999999\",[\"sec2_activity\"])\r\n",
        "\r\n",
        "#employees_number\tIntegerType\tReplace Nulls with integer \"1\". Replace any float number, alpha characters with integer \"1\".\r\n",
        "df = df.na.fill(1,[\"employees_number\"])\r\n",
        "\r\n",
        "#legal_reg_date\tDateType\tReplace Nulls with 1900-1-1. Replace any other value different that DataType to 1900-1-1\r\n",
        "df = df.na.fill(\"1900-1-1\",[\"legal_reg_date\"])\r\n",
        "\r\n",
        "#tax_reg_date\tDateType\tReplace Nulls with 1900-1-1. Replace any other value different that DataType to 1900-1-1\r\n",
        "df = df.na.fill(\"1900-1-1\",[\"tax_reg_date\"])\r\n",
        "\r\n",
        "#e_inv_enroll_date\tDataType\tReplace Nulls with 1900-1-1. Replace any other value different that DataType to 1900-1-1\r\n",
        "df = df.na.fill(\"1900-1-1\",[\"e_inv_enroll_date\"])\r\n",
        "\r\n",
        "#reported_assets\tIntegerType.\tReplace Nulls with value \"0\". Convert any locale value to English equivalent. Replace any other value to \"0\"\r\n",
        "df = df.na.fill(0,[\"reported_assets\"])\r\n",
        "\r\n",
        "#total_capital\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"total_capital\"])\r\n",
        "\r\n",
        "#social_capital\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"social_capital\"])\r\n",
        "\r\n",
        "#total_assets\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"total_assets\"])\r\n",
        "\r\n",
        "#total_fixed_assets\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"total_fixed_assets\"])\r\n",
        "\r\n",
        "#total_liabilities\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"total_liabilities\"])\r\n",
        "\r\n",
        "#gross_income\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"gross_income\"])\r\n",
        "\r\n",
        "#net_income\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"net_income\"])\r\n",
        "\r\n",
        "#total_vat_sales\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"total_vat_sales\"])\r\n",
        "\r\n",
        "#credited_einvoicing_value\tFloatType\tReplace Nulls with Float Number \"1.0\". Replace any alpha value with float number \"1.0\". Covert integers to float.\r\n",
        "df = df.na.fill(1.0,[\"credited_einvoicing_value\"])\r\n",
        "\r\n",
        "#state\tStringType\tReplace Nulls with code \"00\". If there are values in the format of \"006, 0006, 00006, 000006, etc., keep the first two characters from right to left, and remove the rest. If there are values in zero \"0\", \"00\", \"000\", etc., replace with the code \"00\".\r\n",
        "df = df.na.fill(\"00\",[\"state\"])\r\n",
        "\r\n",
        "#municipality\tStringType\tReplace Nulls with \"000000\" code. Covert any other code out of the range of 000001 to 999999 to \"000000\"\r\n",
        "df = df.na.fill(\"000000\",[\"municipality\"])\r\n",
        "\r\n",
        "#city\tStringType\tReplace Nulls with \"000000\" code. Covert any other code out of t\r\n",
        "df = df.na.fill(\"000000\",[\"city\"])"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Check we don't have any null values left \r\n",
        "# Find Count of Null, None, NaN of All DataFrame Columns\r\n",
        "df.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df.columns]\r\n",
        "   ).show(vertical=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write out the clean file to the bronze zone\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Saving cleaned taxpayer profile file to ADLS'):\r\n",
        "    output_file_name = \"tax_payer_profile\"\r\n",
        "    df.write.mode(\"overwrite\").parquet(taxpayer_profile_cleaned_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
