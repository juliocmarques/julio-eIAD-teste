{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 20\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "invoice_schema_applied_path = ''\n",
        "invoice_cleaned_path = ''\n",
        "statistics_path = ''\n",
        "exchange_rates_file = ''\n",
        "data_separator = ''\n",
        "data_encoding = ''\n",
        "drop_records_threshold = 0\n",
        "local_currency_iso_code = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import datetime\n",
        "import csv\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import date\n",
        "from calendar import monthrange\n",
        "import time\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, isnan, when, count, length, trim\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, TimestampType, FloatType"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'statistics_path': statistics_path,\n",
        "    'data_encoding': data_encoding,\n",
        "    'data_separator': data_separator,\n",
        "    'invoice_schema_applied_path': invoice_schema_applied_path,\n",
        "    'invoice_cleaned_path': invoice_cleaned_path,\n",
        "    'exchange_rates_file': exchange_rates_file,\n",
        "    'drop_records_threshold': drop_records_threshold,\n",
        "    'local_currency_iso_code': local_currency_iso_code,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def deep_ls(path: str, max_depth=1):\n",
        "    \"\"\"\n",
        "    List all files and folders in specified path and\n",
        "    subfolders within maximum recursion depth.\n",
        "    \"\"\"\n",
        "\n",
        "    # List all files in path\n",
        "    li = mssparkutils.fs.ls(path)\n",
        "\n",
        "    # Return all files\n",
        "    for x in li:\n",
        "        if x.size != 0:\n",
        "            yield x\n",
        "\n",
        "    # If the max_depth has not been reached, start\n",
        "    # listing files and folders in subdirectories\n",
        "    if max_depth > 1:\n",
        "        for x in li:\n",
        "            if x.size != 0:\n",
        "                continue\n",
        "            for y in deep_ls(x.path, max_depth - 1):\n",
        "                yield y\n",
        "\n",
        "    # If max_depth has been reached,\n",
        "    # return the folders\n",
        "    else:\n",
        "        for x in li:\n",
        "            if x.size == 0:\n",
        "                yield x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_total_nulls(df, colname) :\n",
        "    if [dtype for name, dtype in df.dtypes if name == colname][0] not in (\"timestamp\", \"date\"):\n",
        "        total_rows = df.where(col(colname).contains('None') | col(colname).contains('NULL') | (col(colname) == '' ) | col(colname).isNull() | isnan(colname)).count()\n",
        "    else:\n",
        "        total_rows = df.where(col(colname).contains('None') | col(colname).contains('NULL') | (col(colname) == '' ) | col(colname).isNull()).count()\n",
        "    return total_rows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def apply_cleansing_rules(fileName, fullFilePath, df_exchange_rate) :\n",
        "    from pyspark.sql.functions import col, year, month, dayofmonth, isnan, when, count, length, trim\n",
        "    from pyspark.sql.types import StringType, DateType, FloatType, IntegerType\n",
        "    \n",
        "    df = spark.read.csv(fullFilePath, sep=data_separator,inferSchema=True, header=True)\n",
        "\n",
        "    # Change data types\n",
        "    df = df.withColumn('issuer_type' , df['issuer_type'].cast(StringType()))\n",
        "    df = df.withColumn('issuer_id' , df['issuer_id'].cast(StringType()))\n",
        "    df = df.withColumn('activity_issuer' , df['activity_issuer'].cast(StringType()))\n",
        "    df = df.withColumn('receiver_type' , df['receiver_type'].cast(StringType()))\n",
        "    df = df.withColumn('receiver_id' , df['receiver_id'].cast(StringType()))\n",
        "    df = df.withColumn('document_type' , df['document_type'].cast(StringType()))\n",
        "    df = df.withColumn('document_id' , df['document_id'].cast(StringType()))\n",
        "    df = df.withColumn('issued_date' , df['issued_date'].cast(TimestampType()))\n",
        "    df = df.withColumn('sales_terms' , df['sales_terms'].cast(StringType()))\n",
        "    df = df.withColumn('credit_term' , df['credit_term'].cast(IntegerType()))\n",
        "    df = df.withColumn('currency' , df['currency'].cast(StringType()))\n",
        "    df = df.withColumn('exchange_rate_r' , df['exchange_rate_r'].cast(FloatType()))\n",
        "    df = df.withColumn('payment_method1' , df['payment_method1'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method2' , df['payment_method2'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method3' , df['payment_method3'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method4' , df['payment_method4'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method5' , df['payment_method5'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method5' , df['payment_method5'].cast(StringType()))\n",
        "    df = df.withColumn('payment_method99' , df['payment_method99'].cast(StringType()))\n",
        "    df = df.withColumn('total_taxable_services' , df['total_taxable_services'].cast(FloatType()))\n",
        "    df = df.withColumn('total_non_taxable_services' , df['total_non_taxable_services'].cast(FloatType()))\n",
        "    df = df.withColumn('total_taxable_goods' , df['total_taxable_goods'].cast(FloatType()))\n",
        "    df = df.withColumn('total_non_taxable_goods' , df['total_non_taxable_goods'].cast(FloatType()))\n",
        "    df = df.withColumn('total_taxable' , df['total_taxable'].cast(FloatType()))\n",
        "    df = df.withColumn('total_non_taxable' , df['total_non_taxable'].cast(FloatType()))\n",
        "    df = df.withColumn('total_sales' , df['total_sales'].cast(FloatType()))\n",
        "    df = df.withColumn('total_discounts' , df['total_discounts'].cast(FloatType()))\n",
        "    df = df.withColumn('total_voucher' , df['total_voucher'].cast(FloatType()))\n",
        "    df = df.withColumn('total_tax' , df['total_tax'].cast(FloatType()))\n",
        "\n",
        "    # Remove rows where null values are under the allowed threshold\n",
        "    total_rows_in_dataset = df.count() \n",
        "    logger.info(f'Total rows in dataset to be cleaned: {total_rows_in_dataset}')\n",
        "\n",
        "    #issuer_id.\tDrop records if < 1%.Stop, review, fix if > 1%\n",
        "    total_rows_null_issuer_id = get_total_nulls(df,\"issuer_id\")\n",
        "    logger.info(f'Total rows with null issuer_id: {total_rows_null_issuer_id}')\n",
        "    per_rows_null_issuer_id = total_rows_null_issuer_id / total_rows_in_dataset\n",
        "    if (per_rows_null_issuer_id <= drop_records_threshold) :\n",
        "        df.na.drop(subset=[\"issuer_id\"])\n",
        "\n",
        "    #issued_date. Drop records if < 1%.Stop, review, fix if > 1%\n",
        "    total_rows_null_issued_date = get_total_nulls(df,\"issued_date\")\n",
        "    per_rows_null_issued_date = total_rows_null_issued_date / total_rows_in_dataset\n",
        "    if (per_rows_null_issuer_id <= drop_records_threshold) :\n",
        "        df.na.drop(subset=[\"issued_date\"])    \n",
        "\n",
        "    #total_voucher. Drop records if < 1%.Stop, review, fix if > 1%\n",
        "    total_rows_null_total_voucher = get_total_nulls(df,\"total_voucher\")\n",
        "    per_rows_null_total_voucher = total_rows_null_total_voucher / total_rows_in_dataset\n",
        "    if (per_rows_null_total_voucher <= drop_records_threshold) :\n",
        "        df = df.filter(col(\"total_voucher\").isNotNull() & ~isnan(\"total_voucher\")) \n",
        "\n",
        "    #total_tax. Drop records if < 1%.Stop, review, fix if > 1% \n",
        "    total_rows_null_total_tax = get_total_nulls(df,\"total_tax\")\n",
        "    per_rows_null_total_tax = total_rows_null_total_tax / total_rows_in_dataset\n",
        "    if (per_rows_null_total_tax <= drop_records_threshold) :\n",
        "        df = df.filter(col(\"total_tax\").isNotNull() & ~isnan(\"total_tax\")) \n",
        "\n",
        "\n",
        "    #total_voucher. Replace with 0.0 if Null. Replace iwth 0.0 if any alpha character\n",
        "    df = df.fillna(value=0, subset=['total_voucher'])\n",
        "\n",
        "    #total_tax. Replace with 0.0 if Null. Replace iwth 0.0 if any alpha character\n",
        "    df = df.fillna(value=0, subset=['total_tax'])\n",
        "\n",
        "    #document_id\tString\t8 to 100-character code that identifies the document number.\tSet to “no_identified_document”\n",
        "    df = df.fillna(value=\"no_identified_document\", subset=['document_id'])\n",
        "\n",
        "    #Currency\tString\tCurrency code according to ISO 4217\tSet to local currency\n",
        "    df = df.fillna(value=local_currency_iso_code, subset=['currency'])\n",
        "\n",
        "    #document_type\tstring\tAlphanumeric code that identifies the type of electronic voucher: Example: I: Invoice, D: Debit Note, C: Credit Note, O: Order, G: Goods certificate, T: Tender Receipt, TC: Tender Contract, etc.\tSet to “I” = to Invoice\n",
        "    df = df.fillna(value='I', subset=['document_type'])\n",
        "\n",
        "    #receiver_id\tString\tTax ID associated with the taxpayer who received the e-Invoicing.\tSet to no_identified_receiver\n",
        "    df = df.fillna(value=\"no_identified_receiver\", subset=['receiver_id'])\n",
        "\n",
        "    #receiver_id. Set to no_identified_receiver if any value > 16 characters.\n",
        "    df = df.withColumn(\"receiver_id\", \\\n",
        "              when(length(df[\"receiver_id\"]) > 16, 'no_identified_receiver').otherwise(df[\"receiver_id\"]))\n",
        "\n",
        "    #activity_issuer\tString\tIndicates the code of the economic activity to which the electronic receipt corresponds.\t\n",
        "    #Replace with activiy code \"999999\" if Null.\n",
        "    #Replace with activiy code \"999999\" if lenght > 8 digits.\n",
        "    #Replace with activiy code \"999999\" if length code < 4 digits\n",
        "    \n",
        "    df = df.fillna(value=\"999999\", subset=['activity_issuer'])\n",
        "\n",
        "    df = df.withColumn(\"activity_issuer\", \\\n",
        "              when(length(df[\"activity_issuer\"]) > 8, '999999').otherwise(df[\"activity_issuer\"]))\n",
        "\n",
        "    df = df.withColumn(\"activity_issuer\", \\\n",
        "              when(length(df[\"activity_issuer\"]) < 4, '999999').otherwise(df[\"activity_issuer\"]))\n",
        "\n",
        "\n",
        "    #total_taxable.\tSet to 0 (currency)\n",
        "    df = df.fillna(value=0, subset=['total_taxable'])\n",
        "\n",
        "    #total_non_taxable.\tSet to 0 (currency)\n",
        "    df = df.fillna(value=0, subset=['total_non_taxable'])\n",
        "\n",
        "    #total_sales. Set to 0 (currency)\n",
        "    df = df.fillna(value=0, subset=['total_sales'])\n",
        "\n",
        "    #total_discounts.\tSet to 0 (currency)\n",
        "    df = df.fillna(value=0, subset=['total_discounts'])\n",
        "\n",
        "    #total_taxable_services.\tSet to 0\n",
        "    df = df.fillna(value=0, subset=['total_taxable_services'])\n",
        "\n",
        "    #total_non_taxable_services.\tSet to 0\n",
        "    df = df.fillna(value=0, subset=['total_non_taxable_services'])\n",
        "\n",
        "    #total_taxable_goods. Set to 0\n",
        "    df = df.fillna(value=0, subset=['total_taxable_goods'])\n",
        "\n",
        "    #total_non_taxable_goods. Set to 0\n",
        "    df = df.fillna(value=0, subset=['total_non_taxable_goods'])  \n",
        "  \n",
        "    #issuer_type\tReplace with \"NT\" if Null\n",
        "    df = df.fillna(value=\"NT\", subset=['issuer_type'])\n",
        "\n",
        "    #receiver_type\tReplace with \"NT\" if Null\n",
        "    df = df.fillna(value=\"NT\", subset=['receiver_type'])\n",
        "\n",
        "    #sales_terms\tReplace with \"00\" if Null\n",
        "    df = df.fillna(value=\"00\", subset=['sales_terms'])\n",
        "\n",
        "    #credit_term\tReplace with \"00\" if Null\n",
        "    df = df.fillna(value=\"00\", subset=['credit_term'])\n",
        "\n",
        "    #exchange_rate_r. Replace with 1.0 if null\n",
        "    df = df.fillna(value=1, subset=['exchange_rate_r'])\n",
        "\n",
        "    #payment_method1\tReplace with \"00\" if Null\n",
        "    #payment_method1 Replace with \"00\" if any length > 2\n",
        "    df = df.fillna(value=\"00\", subset=['payment_method1'])\n",
        "    df = df.withColumn(\"payment_method1\", \\\n",
        "            when(length(df[\"payment_method1\"]) > 2, '00').otherwise(df[\"payment_method1\"]))\n",
        "\n",
        "    #payment_method2\tReplace with \"00\" if Null\n",
        "    #payment_method2 Replace with \"00\" if any length > 2\n",
        "    df = df.fillna(value=\"00\", subset=['payment_method2'])\n",
        "    df = df.withColumn(\"payment_method2\", \\\n",
        "            when(length(df[\"payment_method2\"]) > 2, '00').otherwise(df[\"payment_method2\"]))\n",
        "\n",
        "    #payment_method3\tReplace with \"00\" if Null\n",
        "    #payment_method3 Replace with \"00\" if any length > 2\n",
        "    df = df.fillna(value=\"00\", subset=['payment_method3'])\n",
        "    df = df.withColumn(\"payment_method3\", \\\n",
        "            when(length(df[\"payment_method3\"]) > 2, '00').otherwise(df[\"payment_method3\"]))\n",
        "\n",
        "    #payment_method4\tReplace with \"00\" if Null\n",
        "    #payment_method4 Replace with \"00\" if any length > 2\n",
        "    df = df.fillna(value=\"00\", subset=['payment_method4'])\n",
        "    df = df.withColumn(\"payment_method4\", \\\n",
        "            when(length(df[\"payment_method4\"]) > 2, '00').otherwise(df[\"payment_method4\"]))\n",
        "\n",
        "    #payment_method5\tReplace with \"00\" if Null\n",
        "    #payment_method5 Replace with \"00\" if any length > 2\n",
        "    df = df.fillna(value=\"00\", subset=['payment_method5'])\n",
        "    df = df.withColumn(\"payment_method5\", \\\n",
        "            when(length(df[\"payment_method5\"]) > 2, '00').otherwise(df[\"payment_method5\"]))\n",
        "\n",
        "    #payment_method99\tReplace with \"00\" if Null\n",
        "    #payment_method99 Replace with \"00\" if any length > 2\n",
        "    df = df.fillna(value=\"00\", subset=['payment_method99'])\n",
        "    df = df.withColumn(\"payment_method99\", \\\n",
        "            when(length(df[\"payment_method99\"]) > 2, '00').otherwise(df[\"payment_method99\"]))\n",
        "\n",
        "    # merge df and df_er so df has all the exchange rates\n",
        "    \n",
        "    df = df.join(df_exchange_rate, (df['currency'] == df_exchange_rate['currency_exchange_rate']) & (df['issued_date'] == df_exchange_rate['date_exchange_rate']), how='left')    \n",
        "\n",
        "    #set the local currency to rows where the currency does not need to be converted\n",
        "\n",
        "    df = df.fillna(value=local_currency_iso_code, subset=['currency_exchange_rate'])\n",
        "    df = df.fillna(value=1, subset=['exchange_rate_value'])\n",
        "\n",
        "    #remove columns no longer needed\n",
        "\n",
        "    df = df.drop('date_exchange_rate') \n",
        "\n",
        "\n",
        "    # apply exchange rates and create new columns for freshly transformed columns\n",
        "    monetary_cols = ['total_taxable_services',\n",
        "                    'total_tax',\n",
        "                    'total_non_taxable_services',\n",
        "                    'total_non_taxable_goods',\n",
        "                    'total_taxable_goods',\n",
        "                    'total_taxable',\n",
        "                    'total_non_taxable',\n",
        "                    'total_sales',\n",
        "                    'total_discounts',\n",
        "                    'total_voucher']\n",
        "    for col in monetary_cols:\n",
        "        transformed_col = 'transformed_' + col\n",
        "        df = df.withColumn(transformed_col,  df[col]*df['exchange_rate_value'])\n",
        "\n",
        "    #remove columns no longer needed\n",
        "    df = df.drop('currency_exchange_rate') \n",
        "\n",
        "    #return the cleansed dataset for this file\n",
        "    return df\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Data cleaning \n",
        "\n",
        "In this code we read all the loaded files and we apply the cleaning rules, new columns are created using the exchange rate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Loading and parsing exchange rate file'):\n",
        "    # read in exchange rate file\n",
        "    logger.info('Reading in exchange rate dictionary')\n",
        "    df_exchangerate = spark.read.csv(exchange_rates_file, sep=data_separator,inferSchema=True, header=True).toPandas()\n",
        "\n",
        "    logger.info('Extracting exchange rate info')\n",
        "    ignore_cols = ['ISO_CODE','DESCRIPTION']\n",
        "    # only read in exchange rate values\n",
        "    dates = [x for x in df_exchangerate.columns if x not in ignore_cols]\n",
        "    exchange_rate_ll = []\n",
        "\n",
        "    for row in df_exchangerate.iterrows():\n",
        "        # ISO_CODE is basically just currency\n",
        "        country = row[1]['ISO_CODE']\n",
        "        for date in dates:\n",
        "            # modify date format to match that of original df dates\n",
        "            issued_date_parts = str(pd.to_datetime(date).date()).split('-')\n",
        "            issued_date = issued_date_parts[0] + '-' + issued_date_parts[1] + '-' + issued_date_parts[2]\n",
        "            # remove commas from exchange_rate num values (e.g. 1,000)\n",
        "            exchange_rate = float(row[1][date])\n",
        "            exchange_rate_ll.append([country,issued_date,exchange_rate])\n",
        "\n",
        "\n",
        "    df_er = pd.DataFrame(exchange_rate_ll,columns=['currency_exchange_rate','date_exchange_rate','exchange_rate_value'])\n",
        "\n",
        "    #Create PySpark DataFrame from Pandas\n",
        "    df_exchange_rate =spark.createDataFrame(df_er) \n",
        "\n",
        "with tracer.span('Loading schema applied invoice files'):\n",
        "    file_names = mssparkutils.fs.ls(invoice_schema_applied_path)\n",
        "\n",
        "firstFile = True\n",
        "for filename in file_names:  \n",
        "    logger.info(f'Applying cleansing rules for invoice file: {filename.name}')\n",
        "    with tracer.span('Applying cleansing rules for invoice file'):\n",
        "        df_cleansed_file_data = apply_cleansing_rules(filename.name, filename.path, df_exchange_rate)\n",
        "    \n",
        "    if firstFile == True:\n",
        "        df_all_cleansed_data = df_cleansed_file_data\n",
        "        firstFile = False\n",
        "    else:\n",
        "        df_all_cleansed_data = df_all_cleansed_data.union(df_cleansed_file_data)\n",
        "\n",
        "\n",
        "with tracer.span('Saving all cleansed data to ADLS'):\n",
        "    df_all_cleansed_data.write.mode(\"overwrite\").parquet(invoice_cleaned_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
