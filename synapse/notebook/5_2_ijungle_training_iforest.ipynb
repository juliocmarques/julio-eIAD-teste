{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "    \"spark.sql.autoBroadcastJoinThreshold\": -1,\r\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 8,\r\n",
        "    \"spark.rpc.message.maxSize\": 1024\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from pyspark.ml.functions import vector_to_array\r\n",
        "import numpy as np\r\n",
        "import pandas as pd\r\n",
        "from sklearn.ensemble import IsolationForest\r\n",
        "import joblib\r\n",
        "from io import BytesIO\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "from pyspark.sql.types import StructField, StructType, StringType, IntegerType, LongType"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\r\n",
        "prepped_data_path = ''\r\n",
        "iFor_data_prefix = ''\r\n",
        "subsample_list = ''\r\n",
        "trees_list = ''\r\n",
        "train_size = ''\r\n",
        "id_feat = ''\r\n",
        "seed = ''\r\n",
        "time_slice_folder = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\r\n",
        "import logging\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\r\n",
        "from opencensus.trace import config_integration\r\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\r\n",
        "from opencensus.trace.tracer import Tracer\r\n",
        "\r\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\r\n",
        "config_integration.trace_integrations(['logging'])\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\r\n",
        "logger.setLevel(logging.INFO)\r\n",
        "\r\n",
        "tracer = Tracer(\r\n",
        "    exporter=AzureExporter(\r\n",
        "        connection_string=instrumentation_connection_string\r\n",
        "    ),\r\n",
        "    sampler=AlwaysOnSampler()\r\n",
        ")\r\n",
        "\r\n",
        "# Spool parameters\r\n",
        "run_time_parameters = {'custom_dimensions': {\r\n",
        "    'batch_id': batch_id,\r\n",
        "    'prepped_data_path': prepped_data_path,\r\n",
        "    'iFor_data_prefix': iFor_data_prefix,\r\n",
        "    'subsample_list': subsample_list,\r\n",
        "    'trees_list': trees_list,\r\n",
        "    'train_size': train_size,\r\n",
        "    'id_feat': id_feat,\r\n",
        "    'seed': seed,\r\n",
        "    'time_slice_folder': time_slice_folder,\r\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "  \r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if prepped_data_path != \"\":\r\n",
        "    prepped_data_path = \"/\".join(prepped_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + prepped_data_path.split(\"/\")[-1]\r\n",
        "    logger.info(f'prepped_data_path = {prepped_data_path}')\r\n",
        "if iFor_data_prefix != \"\":\r\n",
        "    iFor_data_prefix = \"/\".join(iFor_data_prefix.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + iFor_data_prefix.split(\"/\")[-1]\r\n",
        "    logger.info(f'iFor_data_prefix = {iFor_data_prefix}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Cast parameters\r\n",
        "subsample_list = [int(i) for i in subsample_list.split(\",\")]\r\n",
        "trees_list = [int(i) for i in trees_list.split(\",\")]\r\n",
        "train_size = float(train_size)\r\n",
        "id_feat = [i for i in id_feat.split(\",\")]\r\n",
        "seed = int(seed)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = spark.read.parquet(prepped_data_path)\r\n",
        "m = df.count()\r\n",
        "logger.info(f'Number of records: {m}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Add id to join with group table\r\n",
        "df_id = df.withColumn('_id',F.monotonically_increasing_id())"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def ijungle_train(id_feat, seed):\r\n",
        "    def _fun(key, pdf):\r\n",
        "        trees = key[0]\r\n",
        "        subsample_size = key[1]\r\n",
        "        group = key[2]\r\n",
        "        pdf.set_index(id_feat, inplace=True)\r\n",
        "        feats = list(pdf.columns)\r\n",
        "        feats.remove('_group')\r\n",
        "        feats.remove('_tree_size')\r\n",
        "        feats.remove('_subsample_size')\r\n",
        "        pdf = pdf[feats]\r\n",
        "        clf = IsolationForest(\r\n",
        "            n_estimators = trees, \r\n",
        "            max_samples=min(subsample_size, pdf.shape[0]), \r\n",
        "            random_state=seed, n_jobs=-1)\r\n",
        "        clf.fit(pdf)\r\n",
        "        bytes_container = BytesIO()\r\n",
        "        joblib.dump(clf, bytes_container)\r\n",
        "        bytes_container.seek(0)\r\n",
        "        model_bytes = bytes_container.read()\r\n",
        "        return(pd.DataFrame([(trees, subsample_size, group, model_bytes)]))\r\n",
        "    return(_fun)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_unassembled_exists = False\r\n",
        "num_feats = len(df_id.take(1)[0]['scaled'])\r\n",
        "tot_models = 0\r\n",
        "idx = 0\r\n",
        "for trees in trees_list:\r\n",
        "    for subsample_size in subsample_list:\r\n",
        "        #Get random train_size fraction sample of data\r\n",
        "        df_id_group = df_id.sample(withReplacement=False, fraction=train_size, seed=seed+idx)\r\n",
        "        \r\n",
        "        #Calculate how many groups can fit. Each group is a distinct subset of the data that the model will be trained on.\r\n",
        "        rand_group_count = df_id_group.count()\r\n",
        "        num_groups = int(np.floor(rand_group_count / subsample_size))\r\n",
        "        logger.info(f\"For trees:{trees} and subsample_size:{subsample_size}, the number of rows is {rand_group_count} and the number of groups is {num_groups}.\")\r\n",
        "        tot_models += num_groups\r\n",
        "\r\n",
        "        #Throw a random number for each row, sort by the ranom number, give it a monotonically increasing id and take the modulus of that id against the number of groups\r\n",
        "        #The modulus and the fact that the row numbers are not consecutive numbers will lead to some variation in the number of samples in the groups.\r\n",
        "        #However, this variation will be small and keeps the spirit and intent of the isolation forest methodology intact. Getting the exact number of samples for each model\r\n",
        "        #in a random fashion would be a much more expensive operation, at least for large datasets. \r\n",
        "        df_id_group = df_id_group.withColumn(\"_rand\",F.rand(seed+idx)).select('_id','_rand')\r\n",
        "        df_id_group = df_id_group.orderBy('_rand').withColumn(\"shuffled_index\",F.monotonically_increasing_id()).drop('_rand')\r\n",
        "        df_id_group = df_id_group.withColumn(\"_group\",F.col(\"shuffled_index\") % num_groups)\r\n",
        "        \r\n",
        "        # Join of random selection of groups with training data\r\n",
        "        df_subsamples = df_id.join(df_id_group, on='_id').where(F.col('_group')>=0).select(id_feat+['scaled','_group'])\r\n",
        "        df_subsamples = df_subsamples.withColumn(\"_tree_size\",F.lit(trees)).withColumn(\"_subsample_size\",F.lit(subsample_size))\r\n",
        "        \r\n",
        "        # Vector to individual columns to prepare for parallel training\r\n",
        "        if df_unassembled_exists:\r\n",
        "            df_unassembled = df_unassembled.union(df_subsamples.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + ['_tree_size','_subsample_size','_group'] + [F.col(\"f\")[i] for i in range(num_feats)]))\r\n",
        "        else:\r\n",
        "            df_unassembled = df_subsamples.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + ['_tree_size','_subsample_size','_group'] + [F.col(\"f\")[i] for i in range(num_feats)])\r\n",
        "            df_unassembled_exists = True\r\n",
        "        idx += 1\r\n",
        "logger.info(f'Number of models: {tot_models}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_iFor = df_unassembled.groupBy('_tree_size', '_subsample_size', '_group').applyInPandas(\r\n",
        "    ijungle_train(id_feat, seed), \r\n",
        "    schema=\"tree_size long, subsample_size long, id long, model binary\"\r\n",
        ")\r\n",
        "df_iFor.write.mode('overwrite').parquet(iFor_data_prefix)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
