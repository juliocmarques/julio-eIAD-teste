{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## This notebook creates the edges to calculate some of the graph features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 40\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.ml.feature import StringIndexer\n",
        "from pyspark.sql.functions import col, when\n",
        "import pyspark.sql.functions as F\n",
        "import time\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Load Input Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "invoice_cleaned_path = ''\n",
        "edge_path = ''\n",
        "model_path = ''\n",
        "heatmap_path = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'invoice_cleaned_path': invoice_cleaned_path,\n",
        "    'edge_path': edge_path,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "with tracer.span('Loading cleaned invoice files'):\r\n",
        "    df = spark.read.parquet(invoice_cleaned_path,inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create edges and save output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating edges for dataset'):\n",
        "    indexer = StringIndexer(inputCol='issuer_id', outputCol='issuer_id_indexed', stringOrderType='frequencyDesc', handleInvalid='keep')\n",
        "    model = indexer.fit(df)\n",
        "    model.write().overwrite().save(model_path + '/' + '_feature_engineering_indexer_issuer_id.pkl')\n",
        "    df = model.transform(df)\n",
        "    model.setInputCol(\"receiver_id\")\n",
        "    model.setOutputCol(\"receiver_id_indexed\")\n",
        "    df = model.transform(df)\n",
        "    df = df.withColumn('issuer_id_indexed',col('issuer_id_indexed').cast(\"Integer\"))\n",
        "    df = df.withColumn('receiver_id_indexed',col('receiver_id_indexed').cast(\"Integer\"))\n",
        "    all_issuers = df.groupby('issuer_id_indexed').count()\n",
        "    edges = df.groupby('issuer_id_indexed','receiver_id_indexed').count()\n",
        "    edges = edges.toDF(*['seller','buyer','edge_count'])\n",
        "    edges_trimmed = edges.join(all_issuers,edges['buyer']==all_issuers['issuer_id_indexed']).drop('issuer_id_indexed').drop('count')\n",
        "    edges_trimmed_df = edges_trimmed.toDF(*['issuer_id_indexed','receiver_id_indexed','edge_count'])\n",
        "    edges_trimmed_df = edges_trimmed_df.withColumn(\"issuer_id_equals_receiver_id\",when(col(\"issuer_id_indexed\")==col(\"receiver_id_indexed\"),1).otherwise(0))\n",
        "    edges_trimmed_df = edges_trimmed_df.filter(col(\"issuer_id_equals_receiver_id\")==0).drop(\"issuer_id_equals_receiver_id\")\n",
        "\n",
        "with tracer.span('Saving edges to ADLS'):\n",
        "    edges_trimmed_df.write.mode(\"overwrite\").option(\"header\", \"true\").save(edge_path,format='parquet')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# serverless SQL config\n",
        "import pyodbc\n",
        "database = 'eiad'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SyanpseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def generate_schema_string(dataframe):\n",
        "    schema_string = \"\"\n",
        "    for name in dataframe.schema.fieldNames():\n",
        "        schema_string += \"[\" + name + \"] \"\n",
        "        datatype = str(dataframe.schema[name].dataType.simpleString())\n",
        "        if datatype == 'double': datatype = 'float'\n",
        "        if datatype == 'string': datatype = 'nvarchar(MAX)'\n",
        "        if datatype == 'timestamp': datatype = 'datetime2(7)'\n",
        "        schema_string += datatype + \", \"\n",
        "    return schema_string[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for edges'):\n",
        "    table_name = edge_path.split('/')[3] + '_' + edge_path.split('/')[2].split('@')[0] + '_' + edge_path.split('/')[4]\n",
        "    schema_string = generate_schema_string(edges_trimmed_df)\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "    location = \"/\".join([i for idx, i in enumerate(edge_path.split('/')) if idx > 2])\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            try:\n",
        "                cursor.execute(drop_table_command)\n",
        "            except:\n",
        "                pass\n",
        "            cursor.execute(df_sql_command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create activiy heatmap and save output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating heatmap of dataset'):\n",
        "    df = df.withColumn('DD_code',col('activity_issuer').substr(1,2))\n",
        "    activity_types = df.groupby('issuer_id').agg(F.array_distinct(F.collect_list('DD_code')))\n",
        "    code_receiver_counts = df.groupby('DD_code','receiver_id').count()\n",
        "    code_versus_code = code_receiver_counts.join(activity_types,code_receiver_counts['receiver_id']==activity_types['issuer_id']).collect()\n",
        "\n",
        "    heatmap_cells = {i:{j:0 for j in range(10,100)} for i in range(10,100)}\n",
        "    dd_code_hist = {i:0 for i in range(10,100)}\n",
        "    for c in code_versus_code:\n",
        "        dd_code_hist[int(c[0])] += 1\n",
        "        for d in c[4]:\n",
        "            heatmap_cells[int(c[0])][int(d)] += c[2]\n",
        "\n",
        "    heatmap_list = []\n",
        "    for i in heatmap_cells:\n",
        "        user_dict = {\"DD_issue\":i}\n",
        "        receive_dict = {\"DD_receive_fraction_{}\".format(j):float(heatmap_cells[i][j]/max(1,np.sum([heatmap_cells[i][j] for j in heatmap_cells[i]] ))) for j in heatmap_cells[i] if j < 11}\n",
        "        user_dict.update(receive_dict)\n",
        "        heatmap_list.append(user_dict)\n",
        "\n",
        "    dd_issue = []\n",
        "    dd_receive = []\n",
        "    fraction = []\n",
        "    for row in heatmap_list:\n",
        "        ddi = row['DD_issue']\n",
        "        for key in row:\n",
        "            if 'rec' not in key: continue\n",
        "            ddr = int(key[-2:])\n",
        "            frac = row[key]\n",
        "            dd_issue += [ddi]\n",
        "            dd_receive += [ddr]\n",
        "            fraction += [frac]\n",
        "\n",
        "    heatmap_list = []\n",
        "    for a,b,c in zip(dd_issue, dd_receive, fraction):\n",
        "        heatmap_list += [\n",
        "            {\"issuer_activity_code\":a,\n",
        "            \"receiver_activity_code\":b,\n",
        "            \"fraction\":c }\n",
        "        ]\n",
        "\n",
        "with tracer.span('Saving heatmap to ADLS'):\n",
        "    heatmap_df = spark.createDataFrame(heatmap_list)\n",
        "    heatmap_df.write.mode(\"overwrite\").option(\"header\", \"true\").save(heatmap_path,format='parquet')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for heatmap'):\n",
        "    table_name = heatmap_path.split('/')[3] + '_' + heatmap_path.split('/')[2].split('@')[0] + '_' + heatmap_path.split('/')[4]\n",
        "    schema_string = generate_schema_string(heatmap_df)\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "    location = \"/\".join([i for idx, i in enumerate(heatmap_path.split('/')) if idx > 2])\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            try:\n",
        "                cursor.execute(drop_table_command)\n",
        "            except:\n",
        "                pass\n",
        "            cursor.execute(df_sql_command)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
