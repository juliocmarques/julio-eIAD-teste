{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%%configure -f\r\n",
        "{\r\n",
        "\"conf\": {\r\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.enabled\": true,\r\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\r\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 8,\r\n",
        "    \"spark.driver.maxResultSize\": \"20g\"\r\n",
        "   }\r\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import pyspark.sql.functions as F\r\n",
        "from pyspark.ml.functions import vector_to_array\r\n",
        "from pyspark.sql.functions import udf, struct\r\n",
        "from pyspark.sql.types import FloatType\r\n",
        "import numpy as np\r\n",
        "from io import BytesIO\r\n",
        "import joblib\r\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\r\n",
        "prepped_data_path = ''\r\n",
        "iFor_data_prefix = ''\r\n",
        "overhead_data_path = ''\r\n",
        "overhead_results_prefix = ''\r\n",
        "id_feat = ''\r\n",
        "id_feat_types = ''\r\n",
        "seed = ''\r\n",
        "overhead_size = ''\r\n",
        "time_slice_folder = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\r\n",
        "import logging\r\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\r\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\r\n",
        "from opencensus.trace import config_integration\r\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\r\n",
        "from opencensus.trace.tracer import Tracer\r\n",
        "\r\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\r\n",
        "config_integration.trace_integrations(['logging'])\r\n",
        "\r\n",
        "logger = logging.getLogger(__name__)\r\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\r\n",
        "logger.setLevel(logging.INFO)\r\n",
        "\r\n",
        "tracer = Tracer(\r\n",
        "    exporter=AzureExporter(\r\n",
        "        connection_string=instrumentation_connection_string\r\n",
        "    ),\r\n",
        "    sampler=AlwaysOnSampler()\r\n",
        ")\r\n",
        "\r\n",
        "# Spool parameters\r\n",
        "run_time_parameters = {'custom_dimensions': {\r\n",
        "    'batch_id': batch_id,\r\n",
        "    'prepped_data_path': prepped_data_path,\r\n",
        "    'iFor_data_prefix': iFor_data_prefix,\r\n",
        "    'overhead_data_path': overhead_data_path,\r\n",
        "    'overhead_results_prefix': overhead_results_prefix,\r\n",
        "    'id_feat': id_feat,\r\n",
        "    'id_feat_types': id_feat_types,\r\n",
        "    'seed': seed,\r\n",
        "    'overhead_size': overhead_size,\r\n",
        "    'time_slice_folder': time_slice_folder,\r\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\r\n",
        "} }\r\n",
        "  \r\n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "if prepped_data_path != \"\":\r\n",
        "    prepped_data_path = \"/\".join(prepped_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + prepped_data_path.split(\"/\")[-1]\r\n",
        "    logger.info(f'prepped_data_path = {prepped_data_path}')\r\n",
        "if iFor_data_prefix != \"\":\r\n",
        "    iFor_data_prefix = \"/\".join(iFor_data_prefix.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + iFor_data_prefix.split(\"/\")[-1]\r\n",
        "    logger.info(f'iFor_data_prefix = {iFor_data_prefix}')\r\n",
        "if overhead_data_path != \"\":\r\n",
        "    overhead_data_path = \"/\".join(overhead_data_path.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + overhead_data_path.split(\"/\")[-1]\r\n",
        "    logger.info(f'overhead_data_path = {overhead_data_path}')\r\n",
        "if overhead_results_prefix != \"\":\r\n",
        "    overhead_results_prefix = \"/\".join(overhead_results_prefix.split(\"/\")[:-1]) + \"/\" + time_slice_folder + \"/\" + overhead_results_prefix.split(\"/\")[-1]\r\n",
        "    logger.info(f'overhead_results_prefix = {overhead_results_prefix}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Casting parameters\r\n",
        "id_feat = [i for i in id_feat.split(\",\")]\r\n",
        "id_feat_types = [i for i in id_feat_types.split(\",\")]\r\n",
        "seed = int(seed)\r\n",
        "overhead_size = float(overhead_size)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = spark.read.parquet(prepped_data_path)\r\n",
        "m = df.count()\r\n",
        "logger.info(\"Number of records: {:,}\".format(m))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Creation of overhead sample\r\n",
        "df_W = df.sample(withReplacement=False, fraction=overhead_size, seed=seed)\r\n",
        "df_W.write.mode('overwrite').parquet(overhead_data_path)\r\n",
        "\r\n",
        "num_feats = len(df_W.take(1)[0]['scaled'])\r\n",
        "df_W_unassembled = df_W.withColumn('f', vector_to_array(\"scaled\")).select(id_feat + [F.col(\"f\")[i] for i in range(num_feats)])\r\n",
        "logger.info(\"Number of records of overhead dataset: {:,}\".format(df_W.count()))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#This function takes a dictionary of models and runs them across a pandas dataframe from mapInPandas\r\n",
        "def clf_predict():\r\n",
        "    def _fun(iterator):\r\n",
        "        for pdf in iterator:\r\n",
        "            pdf_out_exists = False\r\n",
        "            for key in clf_dict:\r\n",
        "                clf = clf_dict[key]\r\n",
        "                pdf.set_index([\"issuer_id_indexed\",\"issued_date\"], inplace=True)\r\n",
        "                _predict = clf.score_samples(pdf)\r\n",
        "                pdf.reset_index(drop=False, inplace=True)\r\n",
        "                pdf_temp = pdf[[\"issuer_id_indexed\",\"issued_date\"]]\r\n",
        "                pdf_temp['tree_size'] = [key[0]]*pdf.shape[0]\r\n",
        "                pdf_temp['subsample_size'] = [key[1]]*pdf.shape[0]\r\n",
        "                pdf_temp['group_num'] = [key[2]]*pdf.shape[0]\r\n",
        "                pdf_temp[\"predict\"] = _predict\r\n",
        "                if pdf_out_exists==False:\r\n",
        "                    pdf_out = pdf_temp\r\n",
        "                    pdf_out_exists = True\r\n",
        "                else:\r\n",
        "                    pdf_out = pd.concat([pdf_out,pdf_temp])\r\n",
        "            yield(pdf_out)\r\n",
        "    return(_fun)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df_iFor = spark.read.parquet(iFor_data_prefix) #add select by partition'\r\n",
        "df_iFor = df_iFor.withColumn(\"model_id\",F.monotonically_increasing_id())\r\n",
        "ids = df_iFor.select('model_id').collect()\r\n",
        "ids = sorted(ids)\r\n",
        "num_models = len(ids)\r\n",
        "logger.info(\"Number of models: {:,}\".format(num_models))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#The main idea of this piece of code is: Loading models has overhead. So load many at once. Running models in mapInPandas has overhead, so run many at once.\r\n",
        "#The while loop does both of these things. It loads a 'chunk' of models with one collect, creates a python dictionary with the models and the parameters to the models\r\n",
        "#and runs those models using mapInPandas and the clf_predict function. It also writes out the models for each iteration of the while loop, so that the process DAG\r\n",
        "#doesn't get too complicated if the write happens after each call of mapInPandas.\r\n",
        "#This M factor is very important. M is the number of models loaded into memory at once. Too many and the executors will run out of memory. But too few and the job won't run optimally.\r\n",
        "M = 250\r\n",
        "chunk = 0\r\n",
        "schema = \"issuer_id_indexed integer, issued_date timestamp, tree_size integer, subsample_size integer, group_num integer, predict float\"\r\n",
        "df_predict_exists = False\r\n",
        "while M*chunk < num_models:\r\n",
        "    rows = ids[M*chunk:(chunk+1)*M]\r\n",
        "    chunk_models = df_iFor.where(F.col(\"model_id\") >= rows[0][0]).where(F.col(\"model_id\") <= rows[-1][0]).collect()\r\n",
        "    clf_dict = {}\r\n",
        "    for model in chunk_models:\r\n",
        "        clf_dict[(model.tree_size, model.subsample_size, model.id)] = joblib.load(BytesIO(model.model))\r\n",
        "    df_predict = df_W_unassembled.mapInPandas(clf_predict(), schema = schema)\r\n",
        "    if not df_predict_exists:\r\n",
        "        df_predict_exists = True\r\n",
        "        df_predict.write.mode('overwrite').parquet(overhead_results_prefix)\r\n",
        "    else:\r\n",
        "        try:\r\n",
        "            df_predict.write.mode('append').parquet(overhead_results_prefix)\r\n",
        "        except:\r\n",
        "            logger.info(\"Error in writing df_predict in 5_3.\")\r\n",
        "            logger.info(\"Number of models in chunk: {:,}\".format(len(num_models.keys())))\r\n",
        "            logger.info(\"Models in chunk: {}\".format(model.keys()))\r\n",
        "    chunk += 1"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
