{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## This notebook prepares the raw eInvoice data for the Anomaly Detector "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Load packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 40,\n",
        "    \"spark.rpc.message.maxSize\": 1024\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.ml.feature import StringIndexerModel\n",
        "from pyspark.sql.functions import row_number, lit, col, when, count, sum, countDistinct, desc, unix_timestamp, concat, to_timestamp\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType, FloatType, ShortType\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "invoice_cleaned_path = ''\n",
        "taxpayer_profile_cleaned_path = ''\n",
        "edge_path = ''\n",
        "page_rank_path = ''\n",
        "transformed_data_path = ''\n",
        "model_path = ''\n",
        "time_slice_list = ''\n",
        "depth_of_supply_chain_max_iter = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "time_slices = [i for i in time_slice_list.split(\",\")]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'invoice_cleaned_path': invoice_cleaned_path,\n",
        "    'taxpayer_profile_cleaned_path': taxpayer_profile_cleaned_path,\n",
        "    'edge_path': edge_path,\n",
        "    'page_rank_path': page_rank_path,\n",
        "    'transformed_data_path': transformed_data_path,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Load the data from CSVs in Azure Data Lake Storage into Synapse"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Load cleaned invoice files'):\n",
        "    df = spark.read.parquet(invoice_cleaned_path,inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Load page rank file'):\n",
        "    df_pagerank = spark.read.parquet(page_rank_path,inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Load taxpayer profile file'):\n",
        "    df_profile = spark.read.parquet(taxpayer_profile_cleaned_path,inferSchema=True, header=True)\n",
        "    df_profile = df_profile.withColumn('issuer_id', col('taxpayer_id')).drop('taxpayer_id')\n",
        "    df_profile = df_profile.withColumn('issuer_id', df_profile['issuer_id'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('taxpayer_type', df_profile['taxpayer_type'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('fiscal_condition', df_profile['fiscal_condition'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('regime_name', df_profile['regime_name'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('taxpayer_size', df_profile['taxpayer_size'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('main_activity', df_profile['main_activity'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('sec1_activity', df_profile['sec1_activity'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('sec2_activity', df_profile['sec2_activity'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('employees_number', df_profile['employees_number'].cast(IntegerType()))\n",
        "    df_profile = df_profile.withColumn('legal_reg_date', df_profile['legal_reg_date'].cast(DateType()))\n",
        "    df_profile = df_profile.withColumn('tax_reg_date', df_profile['tax_reg_date'].cast(DateType()))\n",
        "    df_profile = df_profile.withColumn('e_inv_enroll_date', df_profile['e_inv_enroll_date'].cast(DateType()))\n",
        "    df_profile = df_profile.withColumn('reported_assets', df_profile['reported_assets'].cast(IntegerType()))\n",
        "    df_profile = df_profile.withColumn('total_capital', df_profile['total_capital'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('social_capital', df_profile['social_capital'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('total_assets', df_profile['total_assets'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('total_fixed_assets', df_profile['total_fixed_assets'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('total_liabilities', df_profile['total_liabilities'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('gross_income', df_profile['gross_income'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('net_income', df_profile['net_income'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('total_vat_sales', df_profile['total_vat_sales'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('credited_einvoicing_value', df_profile['credited_einvoicing_value'].cast(FloatType()))\n",
        "    df_profile = df_profile.withColumn('state', df_profile['state'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('municipality', df_profile['municipality'].cast(StringType()))\n",
        "    df_profile = df_profile.withColumn('city', df_profile['city'].cast(StringType()))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Load edges file'):\n",
        "    edges_trimmed_df = spark.read.parquet(edge_path,inferSchema=True, header=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Load StringIndexer model'):\n",
        "    model = StringIndexerModel.load(model_path + '/' + '_feature_engineering_indexer_issuer_id.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Transform data for input to Anomaly Detector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Add new columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql(\"set spark.sql.legacy.timeParserPolicy=LEGACY\")\n",
        "for i in ['C','D','I','P','X']:\n",
        "    df = df.withColumn(\"document_type_equals_{}\".format(i), when(df.document_type == i, 1).otherwise(0))\n",
        "df = df.withColumn(\"issuer_id_equals_receiver_id\",when(col(\"issuer_id\")==col(\"receiver_id\"),1).otherwise(0))\n",
        "df = df.withColumn(\"total_voucher_to_self\",when(col(\"issuer_id\")==col(\"receiver_id\"),col(\"transformed_total_voucher\")).otherwise(0))\n",
        "\n",
        "#df = df.withColumn('issuer_id',col('issuer_id').cast(\"Integer\"))\n",
        "#df = df.withColumn('receiver_id',col('receiver_id').cast(\"Integer\"))\n",
        "\n",
        "df = df.withColumn('issued_date', to_timestamp(col('issued_date')))\n",
        "\n",
        "df = df.withColumn('day_of_week', F.dayofweek(col('issued_date')))\n",
        "df = df.withColumn('day_of_month', F.dayofmonth(col('issued_date')))\n",
        "df = df.withColumn('day_of_year', F.dayofyear(col('issued_date')))\n",
        "df = df.withColumn('week_of_year', F.weekofyear(col('issued_date')))\n",
        "df = df.withColumn('month', F.month(col('issued_date')))\n",
        "df = df.withColumn('quarter', F.quarter(col('issued_date')))\n",
        "df = df.withColumn('year', F.year(col('issued_date')))\n",
        "df = df.withColumn('hour', F.hour(col('issued_date')))\n",
        "df = df.withColumn('week', F.date_format('issued_date', 'W'))\n",
        "df = df.withColumn('day', F.dayofmonth(col('issued_date')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Index issuer_id"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Index dataset'):\n",
        "    df = model.transform(df)\n",
        "    df = df.withColumn('issuer_id_indexed',col('issuer_id_indexed').cast(\"Integer\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Filter out events with year less than 1950"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = df.where(F.year('issued_date') >= 1950)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Dataset wide features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Calculate features'):\n",
        "    #First and last issued_date removed so that anomaly detector runs.\n",
        "    df_additional = df.join(df_profile.select('issuer_id','tax_reg_date','e_inv_enroll_date'),on='issuer_id',how='leftouter')\n",
        "    df_additional = df_additional.groupby('issuer_id').agg(\n",
        "        F.min('total_voucher'),\n",
        "        F.max('total_voucher'),\n",
        "        F.min('total_tax'),\n",
        "        F.max('total_tax'),\n",
        "        F.sum('total_voucher'),\n",
        "        #F.min('issued_date'),\n",
        "        #F.max('issued_date'),\n",
        "        F.datediff(F.max('issued_date'),F.min('issued_date')) + 1,\n",
        "        F.datediff(F.min('issued_date'),F.first('tax_reg_date')) + 1,\n",
        "        F.datediff(F.min('issued_date'),F.first('e_inv_enroll_date')) + 1\n",
        "    )\n",
        "    df_additional = df_additional.toDF(\n",
        "        \"issuer_id\", \"min_total_voucher_dataset\", \"max_total_voucher_dataset\", \n",
        "        \"min_total_tax_dataset\",\"max_total_tax_dataset\",\"total_voucher_dataset\",\n",
        "        #\"first_issued_date_dataset\",\"last_issued_date_dataset\",\n",
        "        \"num_days_issuing_dataset\",\"time_to_issue_reg\", \"time_to_issue_enroll\")\n",
        "\n",
        "    df_month = df.groupby('issuer_id','month','year').agg(\n",
        "        F.lit('01'),\n",
        "        F.countDistinct('receiver_id')\n",
        "    )\n",
        "    df_month = df_month.toDF('issuer_id','month','year','day','monthly_total_buyers')\n",
        "    df_month = df_month.withColumn('date',F.concat_ws('-',col('year'),col('month'),col('day')).cast('date'))\n",
        "    df_month = df_month.toDF('issuer_id','month','year','day','monthly_total_buyers','date')\n",
        "    w1 = Window.orderBy('date')\n",
        "    df_month = df_month.withColumn('end_date', F.coalesce(F.add_months(F.lead('date').over(w1),-1),'date'))\n",
        "    df_month = df_month.toDF('issuer_id','month','year','day','monthly_total_buyers','date','end_date')\n",
        "    df_month = df_month.selectExpr(\"issuer_id\", \"\"\"\n",
        "        inline_outer(\n",
        "            transform(\n",
        "                sequence(0,int(months_between(end_date, date)+1)),\n",
        "                i -> (add_months(date,i) as date, IF(i=0,monthly_total_buyers,0) as monthly_total_buyers)\n",
        "            )\n",
        "        )\n",
        "    \"\"\")\n",
        "    df_month = df_month.groupby('issuer_id').agg(\n",
        "        F.sum('monthly_total_buyers'),\n",
        "        F.count('monthly_total_buyers')\n",
        "    )\n",
        "    df_month = df_month.toDF(\"issuer_id\",\"average_monthly_total_buyers\",\"denominator\")\n",
        "    df_month = df_month.withColumn(\"average_monthly_total_buyers\",col('average_monthly_total_buyers')/col('denominator')).drop('denominator')\n",
        "\n",
        "    all_issuers = df.groupby('issuer_id').count()\n",
        "\n",
        "    df_month_r = df.groupby('receiver_id','month','year').agg(\n",
        "        F.lit('01'),\n",
        "        F.countDistinct('issuer_id')\n",
        "    )\n",
        "\n",
        "    df_month_r = df_month_r.toDF('issuer_id','month','year','day','monthly_total_suppliers')\n",
        "    df_month_r = df_month_r.withColumn('date',F.concat_ws('-',col('year'),col('month'),col('day')).cast('date'))\n",
        "    df_month_r = df_month_r.join(all_issuers,on=['issuer_id'],how='inner').drop('count')\n",
        "    w1 = Window.orderBy('date')\n",
        "    df_month_r = df_month_r.withColumn('end_date', F.coalesce(F.add_months(F.lead('date').over(w1),-1),'date'))\n",
        "    df_month_r = df_month_r.toDF('issuer_id','month','year','day','monthly_total_suppliers','date','end_date')\n",
        "    df_month_r = df_month_r.selectExpr(\"issuer_id\", \"\"\"\n",
        "        inline_outer(\n",
        "            transform(\n",
        "                sequence(0,int(months_between(end_date, date))),\n",
        "                i -> (add_months(date,i) as date, IF(i=0,monthly_total_suppliers,0) as monthly_total_suppliers)\n",
        "            )\n",
        "        )\n",
        "    \"\"\")\n",
        "    df_month_r = df_month_r.groupby('issuer_id').agg(\n",
        "        F.sum('monthly_total_suppliers'),\n",
        "        F.count('monthly_total_suppliers')\n",
        "    )\n",
        "    df_month_r = df_month_r.toDF(\"issuer_id\",\"average_monthly_total_suppliers\",\"denominator\")\n",
        "    df_month_r = df_month_r.withColumn(\"average_monthly_total_suppliers\",col('average_monthly_total_suppliers')/col('denominator')).drop('denominator')\n",
        "    df_month = df_month.join(df_month_r,on=['issuer_id'],how='outer').fillna(0)\n",
        "    df_additional = df_additional.join(df_month,on='issuer_id',how='leftouter')\n",
        "\n",
        "    df_month_voucher = df.groupby('issuer_id','month','year').agg(\n",
        "        F.lit('01'),\n",
        "        F.sum('transformed_total_voucher')\n",
        "    )\n",
        "    df_month_voucher = df_month_voucher.toDF('issuer_id','month','year','day','monthly_total_voucher',)\n",
        "    df_month_voucher = df_month_voucher.withColumn('date',F.concat_ws('-',col('year'),col('month'),col('day')).cast('date'))\n",
        "    df_month_voucher = df_month_voucher.toDF('issuer_id','month','year','day','monthly_total_voucher','date')\n",
        "    w1 = Window.orderBy('date')\n",
        "    df_month_voucher = df_month_voucher.withColumn('end_date', F.coalesce(F.add_months(F.lead('date').over(w1),-1),'date'))\n",
        "    df_month_voucher = df_month_voucher.toDF('issuer_id','month','year','day','monthly_total_voucher','date','end_date')\n",
        "    df_month_voucher = df_month_voucher.selectExpr(\"issuer_id\", \"\"\"\n",
        "        inline_outer(\n",
        "            transform(\n",
        "                sequence(0,int(months_between(end_date, date)+1)),\n",
        "                i -> (add_months(date,i) as date, IF(i=0,monthly_total_voucher,0) as monthly_total_voucher)\n",
        "            )\n",
        "        )\n",
        "    \"\"\")\n",
        "    df_month_voucher = df_month_voucher.groupby('issuer_id').agg(\n",
        "        F.sum('monthly_total_voucher'),\n",
        "        F.count('monthly_total_voucher')\n",
        "    )\n",
        "    df_month_voucher = df_month_voucher.toDF(\"issuer_id\",\"average_monthly_total_voucher\",\"denominator\")\n",
        "    df_month_voucher = df_month_voucher.withColumn(\"average_monthly_total_voucher\",col('average_monthly_total_voucher')/col('denominator')).drop('denominator')\n",
        "    df_additional = df_additional.join(df_month_voucher,on='issuer_id',how='leftouter')\n",
        "\n",
        "    df_month_tax = df.groupby('issuer_id','month','year').agg(\n",
        "        F.lit('01'),\n",
        "        F.sum('transformed_total_tax')\n",
        "    )\n",
        "    df_month_tax = df_month_tax.toDF('issuer_id','month','year','day','monthly_total_tax',)\n",
        "    df_month_tax = df_month_tax.withColumn('date',F.concat_ws('-',col('year'),col('month'),col('day')).cast('date'))\n",
        "    df_month_tax = df_month_tax.toDF('issuer_id','month','year','day','monthly_total_tax','date')\n",
        "    w1 = Window.orderBy('date')\n",
        "    df_month_tax = df_month_tax.withColumn('end_date', F.coalesce(F.add_months(F.lead('date').over(w1),-1),'date'))\n",
        "    df_month_tax = df_month_tax.toDF('issuer_id','month','year','day','monthly_total_tax','date','end_date')\n",
        "    df_month_tax = df_month_tax.selectExpr(\"issuer_id\", \"\"\"\n",
        "        inline_outer(\n",
        "            transform(\n",
        "                sequence(0,int(months_between(end_date, date)+1)),\n",
        "                i -> (add_months(date,i) as date, IF(i=0,monthly_total_tax,0) as monthly_total_tax)\n",
        "            )\n",
        "        )\n",
        "    \"\"\")\n",
        "    df_month_tax = df_month_tax.groupby('issuer_id').agg(\n",
        "        F.sum('monthly_total_tax'),\n",
        "        F.count('monthly_total_tax')\n",
        "    )\n",
        "    df_month_tax = df_month_tax.toDF(\"issuer_id\",\"average_monthly_total_tax\",\"denominator\")\n",
        "    df_month_tax = df_month_tax.withColumn(\"average_monthly_total_tax\",col('average_monthly_total_tax')/col('denominator')).drop('denominator')\n",
        "    df_additional = df_additional.join(df_month_tax,on='issuer_id',how='leftouter')\n",
        "\n",
        "    df_day_voucher = df.groupby('issuer_id','month','year','day').agg(\n",
        "        F.sum('transformed_total_voucher')\n",
        "    )\n",
        "    df_day_voucher = df_day_voucher.toDF('issuer_id','month','year','day','daily_total_voucher')\n",
        "    df_day_voucher = df_day_voucher.withColumn('date',F.concat_ws('-',col('year'),col('month'),col('day')).cast('date'))\n",
        "    df_day_voucher = df_day_voucher.toDF('issuer_id','month','year','day','daily_total_voucher','date')\n",
        "    w1 = Window.orderBy('date')\n",
        "    df_day_voucher = df_day_voucher.withColumn('end_date', F.coalesce(F.add_months(F.lead('date').over(w1),-1),'date'))\n",
        "    df_day_voucher = df_day_voucher.toDF('issuer_id','month','year','day','daily_total_voucher','date','end_date')\n",
        "    df_day_voucher = df_day_voucher.selectExpr(\"issuer_id\", \"\"\"\n",
        "        inline_outer(\n",
        "            transform(\n",
        "                sequence(0,int(datediff(end_date, date)+1)),\n",
        "                i -> (date_add(date,i) as date, IF(i=0,daily_total_voucher,0) as daily_total_voucher)\n",
        "            )\n",
        "        )\n",
        "    \"\"\")\n",
        "    df_day_voucher = df_day_voucher.groupby('issuer_id').agg(\n",
        "        F.sum('daily_total_voucher'),\n",
        "        F.count('daily_total_voucher')\n",
        "    )\n",
        "    df_day_voucher = df_day_voucher.toDF(\"issuer_id\",\"average_daily_total_voucher\",\"denominator\")\n",
        "    df_day_voucher = df_day_voucher.withColumn(\"average_daily_total_voucher\",col('average_daily_total_voucher')/col('denominator')).drop('denominator')\n",
        "    df_additional = df_additional.join(df_day_voucher,on='issuer_id',how='leftouter')\n",
        "\n",
        "    df_day_tax = df.groupby('issuer_id','month','year','day').agg(\n",
        "        F.sum('transformed_total_tax')\n",
        "    )\n",
        "    df_day_tax = df_day_tax.toDF('issuer_id','month','year','day','daily_total_tax')\n",
        "    df_day_tax = df_day_tax.withColumn('date',F.concat_ws('-',col('year'),col('month'),col('day')).cast('date'))\n",
        "    df_day_tax = df_day_tax.toDF('issuer_id','month','year','day','daily_total_tax','date')\n",
        "    w1 = Window.orderBy('date')\n",
        "    df_day_tax = df_day_tax.withColumn('end_date', F.coalesce(F.add_months(F.lead('date').over(w1),-1),'date'))\n",
        "    df_day_tax = df_day_tax.toDF('issuer_id','month','year','day','daily_total_tax','date','end_date')\n",
        "    df_day_tax = df_day_tax.selectExpr(\"issuer_id\", \"\"\"\n",
        "        inline_outer(\n",
        "            transform(\n",
        "                sequence(0,int(datediff(end_date, date)+1)),\n",
        "                i -> (date_add(date,i) as date, IF(i=0,daily_total_tax,0) as daily_total_tax)\n",
        "            )\n",
        "        )\n",
        "    \"\"\")\n",
        "    df_day_tax = df_day_tax.groupby('issuer_id').agg(\n",
        "        F.sum('daily_total_tax'),\n",
        "        F.count('daily_total_tax')\n",
        "    )\n",
        "    df_day_tax = df_day_tax.toDF(\"issuer_id\",\"average_daily_total_tax\",\"denominator\")\n",
        "    df_day_tax = df_day_tax.withColumn(\"average_daily_total_tax\",col('average_daily_total_tax')/col('denominator')).drop('denominator')\n",
        "    df_additional = df_additional.join(df_day_tax,on='issuer_id',how='leftouter')\n",
        "\n",
        "    df_month12_week4 = df.where((df.week==4) & (df.month==12)).groupby('issuer_id').agg(\n",
        "        F.sum('transformed_total_voucher')\n",
        "    ).toDF('issuer_id','week4_month12_total_voucher')\n",
        "    df_month12_week4_r = df.groupby('receiver_id').agg(\n",
        "        F.sum('transformed_total_voucher')\n",
        "    ).toDF('issuer_id','week4_month12_total_purchase')\n",
        "    df_month12_week4_r = df_month12_week4_r.join(all_issuers,on=['issuer_id'],how='inner').drop('count')\n",
        "    df_month12_week4 = df_month12_week4.join(df_month12_week4_r,on='issuer_id',how='outer').fillna(0)\n",
        "    df_additional = df_additional.join(df_month12_week4, on='issuer_id',how='leftouter').fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Code to create additional features, but these features are at transaction level and so are not included yet\n",
        "'''\n",
        "df_dd = df.join(df_profile.select(\"issuer_id\",\"main_activity\",\"sec1_activity\",\"sec2_activity\"), on='issuer_id',how=\"leftouter\")\n",
        "df_r = df_profile.select(\"issuer_id\",\"main_activity\",\"sec1_activity\",\"sec2_activity\").toDF(\"receiver_id\",\"main_activity_r\",\"sec1_activity_r\",\"sec2_activity_r\")\n",
        "df_dd = df_dd.join(df_r,on=\"receiver_id\",how=\"leftouter\")\n",
        "df_dd = df_dd.withColumn('DD_code',col('activity_issuer').substr(1,2))\n",
        "df_dd = df_dd.withColumn('DD_code_main',col('main_activity').substr(1,2))\n",
        "df_dd = df_dd.withColumn('DD_code_sec1',col('sec1_activity').substr(1,2))\n",
        "df_dd = df_dd.withColumn('DD_code_sec2',col('sec2_activity').substr(1,2))\n",
        "df_dd = df_dd.withColumn('DD_code_main_r',col('main_activity_r').substr(1,2))\n",
        "df_dd = df_dd.withColumn('DD_code_sec1_r',col('sec1_activity_r').substr(1,2))\n",
        "df_dd = df_dd.withColumn('DD_code_sec2_r',col('sec2_activity_r').substr(1,2))\n",
        "df_dd = df_dd.withColumn(\"activity_variation\",when((col(\"DD_code\")==col(\"DD_code_main_r\")) | (col(\"DD_code\")==col(\"DD_code_sec1_r\")) | (col(\"DD_code\")==col(\"DD_code_sec2_r\")) ,0).otherwise(1))\n",
        "df_dd = df_dd.withColumn(\"self_activity_variation\",when((col(\"DD_code\")==col(\"DD_code_main\")) | (col(\"DD_code\")==col(\"DD_code_sec1\")) | (col(\"DD_code\")==col(\"DD_code_sec2\")) ,0).otherwise(1))\n",
        "df_dd = df_dd.select(\"issuer_id\",\"receiver_id\",\"issued_date\",\"DD_code\",\"activity_variation\",\"self_activity_variation\")\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Run groupby / aggregate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#time aggregations\n",
        "aggregation_dict = {\n",
        "    'by_hour':['hour','day_of_month','month','year'],\n",
        "    'by_day':['day_of_month','month','year'],\n",
        "    'by_week':['week_of_year','year'],\n",
        "    'by_month':['month','year'],\n",
        "    'by_quarter':['quarter','year'],\n",
        "    'by_year':['year']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "transformed_dfs = {}\n",
        "for key in time_slices:\n",
        "    issuer_groupby_item = ['issuer_id'] + aggregation_dict[key]\n",
        "    transformed_data_issuer = df.groupby(*issuer_groupby_item).agg(\n",
        "        F.first('issued_date'),\n",
        "        F.count('document_type'),\n",
        "        F.countDistinct('receiver_id'),\n",
        "        F.sum('document_type_equals_C'),\n",
        "        F.sum('document_type_equals_D'),\n",
        "        F.sum('document_type_equals_I'),\n",
        "        F.sum('document_type_equals_P'),\n",
        "        F.sum('document_type_equals_X'),\n",
        "        F.sum('issuer_id_equals_receiver_id'),\n",
        "        F.sum('total_voucher_to_self'),\n",
        "        F.sum('transformed_total_taxable_services'),\n",
        "        F.sum('transformed_total_non_taxable_services'),\n",
        "        F.sum('transformed_total_taxable_goods'),\n",
        "        F.sum('transformed_total_non_taxable_goods'),\n",
        "        F.sum('transformed_total_taxable'),\n",
        "        F.sum('transformed_total_non_taxable'),\n",
        "        F.sum('transformed_total_sales'),\n",
        "        F.sum('transformed_total_discounts'),\n",
        "        F.sum('transformed_total_voucher'),\n",
        "        F.sum('transformed_total_tax'),\n",
        "        F.min('transformed_total_voucher'),\n",
        "        F.max('transformed_total_voucher'),\n",
        "        F.min('transformed_total_tax'),\n",
        "        F.max('transformed_total_tax'),\n",
        "        #F.min('issued_date'),\n",
        "        #F.max('issued_date'),\n",
        "        F.datediff(F.max('issued_date'),F.min('issued_date')) + 1,\n",
        "        F.first('issuer_id_indexed')\n",
        "    )\n",
        "    \n",
        "    all_issuers = df.groupby('issuer_id').count()\n",
        "\n",
        "    receiver_groupby_item = ['receiver_id'] + aggregation_dict[key]\n",
        "\n",
        "    transformed_data_receiver = df.groupby(*receiver_groupby_item).agg(\n",
        "        F.first('issued_date'),\n",
        "        count('document_type'),\n",
        "        countDistinct('issuer_id'),\n",
        "        sum('transformed_total_voucher')\n",
        "    )\n",
        "\n",
        "    #I = Invoice, D = Debit Note, C = Credit Note, O = Order, P = Purchase, G = Goods certificate, T = Tender, X: Export Invoice\n",
        "\n",
        "    #total_invoices: where document_type==I\n",
        "    #total_credit_notes: where document_type==C\n",
        "    #total_debit_notes: where document_type==D\n",
        "    #total_export_invoice: where document_type==X\n",
        "\n",
        "    new_col_names_issuer = ['issuer_id']\n",
        "    new_col_names_issuer +=  aggregation_dict[key]\n",
        "    new_col_names_issuer += [\n",
        "        'issued_date',\n",
        "        'number_of_transactions', 'total_buyers',\n",
        "        'total_credit_notes', 'total_debit_notes',\n",
        "        'total_invoices', 'total_purchase_invoices',\n",
        "        'total_export_invoices', 'number_of_transactions_to_self', 'total_voucher_to_self',\n",
        "        'total_taxable_services', 'total_non_taxable_services',\n",
        "        'total_taxable_goods', 'total_non_taxable_goods',\n",
        "        'total_taxable', 'total_non_taxable',\n",
        "        'total_sales', 'total_discounts',\n",
        "        'total_voucher','total_tax',\n",
        "        'min_total_voucher_period','max_total_voucher_period',\n",
        "        'min_total_tax_period', 'max_total_tax_period',\n",
        "        #'first_issued_date_period','last_issued_date_period',\n",
        "        'num_days_issuing_period','issuer_id_indexed'\n",
        "    ]\n",
        "\n",
        "    new_col_names_receiver = ['issuer_id']\n",
        "    new_col_names_receiver +=  aggregation_dict[key]\n",
        "    new_col_names_receiver += [\n",
        "        'issued_date_r',\n",
        "        'number_of_purchases', 'total_suppliers',\n",
        "        'total_purchases'\n",
        "    ]\n",
        "\n",
        "    transformed_data_issuer = transformed_data_issuer.toDF(*new_col_names_issuer)\n",
        "    transformed_data_receiver = transformed_data_receiver.toDF(*new_col_names_receiver)\n",
        "    transformed_data_receiver_trimmed = transformed_data_receiver.join(all_issuers, on=['issuer_id'],how='inner').drop('count')\n",
        "    transformed_data_receiver_trimmed = model.transform(transformed_data_receiver_trimmed)\n",
        "    transformed_data_receiver_trimmed = transformed_data_receiver_trimmed.withColumn('receiver_id_indexed',col('issuer_id_indexed').cast(\"Integer\")).drop(\"issuer_id_indexed\")\n",
        "    transformed_data = transformed_data_issuer.join(transformed_data_receiver_trimmed, on=['issuer_id']+aggregation_dict[key],how='outer')\n",
        "    transformed_data = transformed_data.withColumn('issuer_id_indexed', when(F.isnull('issuer_id_indexed'), col('receiver_id_indexed')).otherwise(col('issuer_id_indexed'))).drop('receiver_id_indexed')\n",
        "    transformed_data = transformed_data.withColumn('issuer_id',col('issuer_id').cast(\"String\"))\n",
        "    transformed_data = transformed_data.withColumn('issued_date',F.coalesce(col('issued_date'),col('issued_date_r')))\n",
        "    transformed_data = transformed_data.drop(*aggregation_dict[key]).drop(\"issued_date_r\")\n",
        "    transformed_data = transformed_data.fillna(0)\n",
        "    transformed_data = transformed_data.join(df_pagerank,on=['issuer_id_indexed'],how='outer').fillna(0,'pagerank_score')\n",
        "    transformed_data = transformed_data.join(df_additional,on=['issuer_id'],how='leftouter')\n",
        "    transformed_dfs[key] = transformed_data.join(df_profile,on=['issuer_id'],how='leftouter').fillna(0,'employees_number')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Calculate Ratios"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for key in transformed_dfs:\n",
        "    transformed_data = transformed_dfs[key]\n",
        "    transformed_data = transformed_data.withColumn(\"ratio_sales_purchases\", when(col('total_purchases')>0, col('total_sales')/col('total_purchases')).otherwise(col('total_sales')))\n",
        "    transformed_data = transformed_data.withColumn(\"ratio_tax_sales\", when(col('total_sales')>0, col('total_tax')/col('total_sales')).otherwise(col('total_tax')))\n",
        "    transformed_data = transformed_data.withColumn(\"ratio_sales_employees\", when(col('employees_number')>0, col('total_sales')/col('employees_number')).otherwise(col('total_sales')/0.1))\n",
        "    transformed_data = transformed_data.withColumn(\"ratio_buyers_suppliers\", when(col('total_suppliers')>0, col('total_buyers')/col('total_suppliers')).otherwise(col('total_buyers')))\n",
        "    transformed_data = transformed_data.withColumn(\"ratio_sales_capital\", when(col('social_capital')>0, col('total_voucher_dataset')/col('social_capital')).otherwise(col('total_voucher_dataset')))\n",
        "    transformed_dfs[key] = transformed_data.withColumn(\"ratio_in_out\", when(col('number_of_purchases')>0, col('number_of_transactions')/col('number_of_purchases')).otherwise(col('number_of_transactions')))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Economic Activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df2 = df.groupby('issuer_id','activity_issuer').agg(sum('total_voucher'))\n",
        "w2 = Window.partitionBy(\"issuer_id\").orderBy(desc(\"sum(total_voucher)\"))\n",
        "df3 = df2.withColumn(\"row\",row_number().over(w2)).filter(col(\"row\") == 1).drop('row').toDF(\"issuer_id\",\"act01\",\"total_voucher_act01\")\n",
        "df3 = df3.join(df2.withColumn(\"row\",row_number().over(w2)).filter(col(\"row\") == 2).drop('row').toDF(\"issuer_id\",\"act02\",\"total_voucher_act02\"),on=['issuer_id'],how='outer').fillna(0)\n",
        "df3 = df3.join(df2.withColumn(\"row\",row_number().over(w2)).filter(col(\"row\") == 3).drop('row').toDF(\"issuer_id\",\"act03\",\"total_voucher_act03\"),on=['issuer_id'],how='outer').fillna(0)\n",
        "df3 = df3.join(df2.withColumn(\"row\",row_number().over(w2)).filter(col(\"row\") == 4).drop('row').toDF(\"issuer_id\",\"act04\",\"total_voucher_act04\"),on=['issuer_id'],how='outer').fillna(0)\n",
        "df3 = df3.join(df2.withColumn(\"row\",row_number().over(w2)).filter(col(\"row\") == 5).drop('row').toDF(\"issuer_id\",\"act05\",\"total_voucher_act05\"),on=['issuer_id'],how='outer').fillna(0)\n",
        "df3 = df3.na.fill('0')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for key in transformed_dfs:\n",
        "    transformed_dfs[key] = transformed_dfs[key].join(df3,on=['issuer_id'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### In-Memory Depth of Supply Chain Calculation/Network Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "node_child_df = edges_trimmed_df.groupby(\"issuer_id_indexed\").agg(F.collect_list(\"receiver_id_indexed\")).toPandas()\n",
        "node_child_df.columns = ['parent', 'child']\n",
        "logger.info(f'node_child_df.shape: {node_child_df.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#DFS cycle removal algorithm\n",
        "def find_edges_to_remove(df):\n",
        "    edges_to_remove = []\n",
        "    visited = {}\n",
        "    for node in df.parent:\n",
        "        if node not in visited:\n",
        "            visited[node] = 0\n",
        "        if visited[node] == 0:\n",
        "            stack = [(node,0)]\n",
        "            visited[node] = 1\n",
        "            while len(stack) > 0:\n",
        "                t, op = stack.pop()\n",
        "                if op == 0:\n",
        "                    stack.append((t,1))\n",
        "                    #Exception handling for case where issuer has no receivers in trimmed data (final retailer)\n",
        "                    try:\n",
        "                        children = df[df.parent == t].child.tolist()[0]\n",
        "                    except:\n",
        "                        children = []\n",
        "                    for child in children:\n",
        "                        if child not in visited:\n",
        "                            visited[child] = 0\n",
        "                        if visited[child] == 1:\n",
        "                            edges_to_remove.append((int(t),int(child)))\n",
        "                        else:\n",
        "                            if visited[child] == 0:\n",
        "                                stack.append((child,0))\n",
        "                                visited[child] = 1\n",
        "                else:\n",
        "                    visited[t] = 2\n",
        "    return edges_to_remove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "edges_to_remove = find_edges_to_remove(node_child_df)\n",
        "del node_child_df\n",
        "\n",
        "schema = StructType([\n",
        "    StructField(\"issuer_id_indexed\", IntegerType(), True),\n",
        "    StructField(\"receiver_id_indexed\", IntegerType(), True)\n",
        "])\n",
        "remove_df = spark.createDataFrame(edges_to_remove, schema)\n",
        "del edges_to_remove"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "edges_dag = edges_trimmed_df.join(remove_df, on=[\"issuer_id_indexed\",\"receiver_id_indexed\"],how='leftanti').select([\"issuer_id_indexed\",\"receiver_id_indexed\"]).toDF(\"I\",\"R\")\n",
        "edges_dag = edges_dag.withColumn(\"V\",F.lit(1))"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def calculate_left_sc_length_pyspark_max(df, max_iter=2):\n",
        "    '''\n",
        "    Expected columns are I, R, and V. V starts out at 1.\n",
        "    \n",
        "    Returns:\n",
        "        pyspark dataframe with columns node, sc_left. columns that are zero-length are left out, but can be recovered via a join and fillna\n",
        "    '''\n",
        "    left_matrix = df\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        if i > max_iter: break\n",
        "        left_vector = left_matrix.groupBy('R').agg(F.max('V')+1).toDF('R_new','V_new')\n",
        "        left_matrix = df.join(left_vector,df.I==left_vector.R_new,how=\"leftouter\").fillna(0)\n",
        "        left_matrix = left_matrix.withColumn('V',F.when(F.col('V_new')==0,F.col('V')).otherwise(F.col('V_new'))).drop(\"R_new\").drop(\"V_new\")\n",
        "    sample_left_length = left_matrix.groupBy('R').agg(F.max('V')).toDF('node','sc_left')\n",
        "    return sample_left_length\n",
        "\n",
        "def calculate_right_sc_length_pyspark_max(df, max_iter=2):\n",
        "    '''\n",
        "    Expected columns are I, R, and V. V starts out at 1.\n",
        "\n",
        "    Returns:\n",
        "        pyspark dataframe with columns node, sc_right. columns that are zero-length are left out, but can be recovered via a join and fillna\n",
        "    '''\n",
        "    right_matrix = df\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        if i > max_iter: break\n",
        "        right_vector = right_matrix.groupBy('I').agg(F.max('V')+1).toDF('I_new','V_new')\n",
        "        right_matrix = df.join(right_vector,df.R==right_vector.I_new,how=\"leftouter\").fillna(0)\n",
        "        right_matrix = right_matrix.withColumn('V',F.when(F.col('V_new')==0,F.col('V')).otherwise(F.col('V_new'))).drop(\"I_new\").drop(\"V_new\")\n",
        "    sample_right_length = right_matrix.groupBy('I').agg(F.max('V')).toDF('node','sc_right')\n",
        "    return sample_right_length\n",
        "\n",
        "def calculate_left_sc_length_pyspark_min(df, max_iter=2):\n",
        "    '''\n",
        "    Expected columns are I, R, and V. V starts out at 1.\n",
        "    \n",
        "    Returns:\n",
        "        pyspark dataframe with columns node, sc_left. columns that are zero-length are left out, but can be recovered via a join and fillna\n",
        "    '''\n",
        "    left_matrix = df\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        if i > max_iter: break\n",
        "        left_vector = left_matrix.groupBy('R').agg(F.min('V')+1).toDF('R_new','V_new')\n",
        "        left_matrix = df.join(left_vector,df.I==left_vector.R_new,how=\"leftouter\").fillna(0)\n",
        "        left_matrix = left_matrix.withColumn('V',F.when(F.col('V_new')==0,F.col('V')).otherwise(F.col('V_new'))).drop(\"R_new\").drop(\"V_new\")\n",
        "    sample_left_length = left_matrix.groupBy('R').agg(F.min('V')).toDF('node','sc_left')\n",
        "    return sample_left_length\n",
        "\n",
        "def calculate_right_sc_length_pyspark_min(df, max_iter=2):\n",
        "    '''\n",
        "    Expected columns are I, R, and V. V starts out at 1.\n",
        "\n",
        "    Returns:\n",
        "        pyspark dataframe with columns node, sc_right. columns that are zero-length are left out, but can be recovered via a join and fillna\n",
        "    '''\n",
        "    right_matrix = df\n",
        "    i = 0\n",
        "    while True:\n",
        "        i += 1\n",
        "        if i > max_iter: break\n",
        "        right_vector = right_matrix.groupBy('I').agg(F.min('V')+1).toDF('I_new','V_new')\n",
        "        right_matrix = df.join(right_vector,df.R==right_vector.I_new,how=\"leftouter\").fillna(0)\n",
        "        right_matrix = right_matrix.withColumn('V',F.when(F.col('V_new')==0,F.col('V')).otherwise(F.col('V_new'))).drop(\"I_new\").drop(\"V_new\")\n",
        "    sample_right_length = right_matrix.groupBy('I').agg(F.min('V')).toDF('node','sc_right')\n",
        "    return sample_right_length\n",
        "\n",
        "def calculate_sc_length(edges_df, max_iter=-1):\n",
        "    sc_left_min = calculate_left_sc_length_pyspark_min(edges_df,max_iter)\n",
        "    sc_right_min = calculate_right_sc_length_pyspark_min(edges_df,max_iter)\n",
        "    sc_min = sc_left_min.join(sc_right_min,'node','outer').fillna(0)\n",
        "    sc_min = sc_min.withColumn('length',F.col('sc_left')+F.col('sc_right'))\n",
        "    sc_min = sc_min.withColumn('frac',F.col(\"sc_left\")/F.col('length'))\n",
        "    sc_min = sc_min.toDF('issuer_id_indexed','min_distance_from_supplier','min_distance_from_customer','min_depth_of_supply_chain','min_place_in_supply_chain')\n",
        "    sc_left_max = calculate_left_sc_length_pyspark_max(edges_df,max_iter)\n",
        "    sc_right_max = calculate_right_sc_length_pyspark_max(edges_df,max_iter)\n",
        "    sc_max = sc_left_max.join(sc_right_max,'node','outer').fillna(0)\n",
        "    sc_max = sc_max.withColumn('length',F.col('sc_left')+F.col('sc_right'))\n",
        "    sc_max = sc_max.withColumn('frac',F.col(\"sc_left\")/F.col('length'))\n",
        "    sc_max = sc_max.toDF('issuer_id_indexed','max_distance_from_supplier','max_distance_from_customer','max_depth_of_supply_chain','max_place_in_supply_chain')\n",
        "    return sc_max.join(sc_min,'issuer_id_indexed')"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "nodes = calculate_sc_length(edges_dag, max_iter=depth_of_supply_chain_max_iter)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for key in transformed_dfs:\n",
        "    transformed_dfs[key] = transformed_dfs[key].join(nodes,on=['issuer_id_indexed'],how='leftouter')\n",
        "    transformed_dfs[key] = transformed_dfs[key].fillna(\n",
        "        value=0,\n",
        "        subset=['min_distance_from_supplier','min_distance_from_customer','min_depth_of_supply_chain','min_place_in_supply_chain',\n",
        "                'max_distance_from_supplier','max_distance_from_customer','max_depth_of_supply_chain','max_place_in_supply_chain']\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Run job and save output to container"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for key in transformed_dfs:\n",
        "    with tracer.span(f'Saving features for {key} aggregation to ADLS'):\n",
        "        transformed_data = transformed_dfs[key]\n",
        "        transformed_data.write.mode(\"overwrite\").option(\"header\", \"true\").save(transformed_data_path + '/' + key,format='parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Create external tables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# serverless SQL config\n",
        "import pyodbc\n",
        "database = 'eiad'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SyanpseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def generate_schema_string(dataframe):\n",
        "    schema_string = \"\"\n",
        "    for name in dataframe.schema.fieldNames():\n",
        "        schema_string += \"[\" + name + \"] \"\n",
        "        datatype = str(dataframe.schema[name].dataType.simpleString())\n",
        "        if datatype == 'double': datatype = 'float'\n",
        "        if datatype == 'string': datatype = 'nvarchar(MAX)'\n",
        "        if datatype == 'timestamp': datatype = 'datetime2(7)'\n",
        "        schema_string += datatype + \", \"\n",
        "    return schema_string[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for key in transformed_dfs:\n",
        "    with tracer.span(f'Creating SQL table for features of agregation: {key} '):\n",
        "        path = transformed_data_path + '/' + key\n",
        "        table_name = path.split('/')[3] + '_' + path.split('/')[2].split('@')[0] + '_' + path.split('/')[4] + '_' + key\n",
        "        schema_string = generate_schema_string(transformed_dfs[key])\n",
        "        drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "        location = \"/\".join([i for idx, i in enumerate(path.split('/')) if idx > 2])\n",
        "        df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "        with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "            with conn.cursor() as cursor:\n",
        "                try:\n",
        "                    cursor.execute(drop_table_command)\n",
        "                except:\n",
        "                    pass\n",
        "                cursor.execute(df_sql_command)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
