{
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": []
      },
      "source": [
        "%%configure -f\n",
        "{\n",
        "\"conf\": {\n",
        "    \"spark.dynamicAllocation.disableIfMinMaxNotSpecified.enabled\": true,\n",
        "    \"spark.dynamicAllocation.enabled\": true,\n",
        "    \"spark.dynamicAllocation.minExecutors\": 2,\n",
        "    \"spark.dynamicAllocation.maxExecutors\": 40\n",
        "   }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "batch_id = ''\n",
        "invoice_schema_applied_path = ''\n",
        "statistics_path = ''\n",
        "data_separator = ''\n",
        "data_encoding = ''\n",
        "taxpayer_profile_schema_applied_path = ''\n",
        "data_quality_min_date = ''\n",
        "data_quality_max_date = ''"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#Sample Quality Index Weights\r\n",
        "SQi_below_range_weight = 3\r\n",
        "SQi_above_range_weight = 3\r\n",
        "SQi_in_range_weight = 2\r\n",
        "\r\n",
        "#Completeness Index Weights. \r\n",
        "#Increasing the value for one column will increase its importance relative to the other columns being measured for completeness.\r\n",
        "Ci_issuer_id_weight = 3\r\n",
        "Ci_issued_date_weight = 3\r\n",
        "Ci_issuer_type_weight = 2\r\n",
        "Ci_document_type_weight = 2\r\n",
        "Ci_currency_weight = 2\r\n",
        "Ci_exchange_rate_weight = 1\r\n",
        "Ci_total_taxable_weight = 1\r\n",
        "Ci_total_non_taxable_weight = 1\r\n",
        "Ci_total_sales_weight = 1\r\n",
        "Ci_total_voucher_weight = 2\r\n",
        "Ci_total_tax_weight = 2\r\n",
        "Ci_activity_issuer_weight = 2\r\n",
        "Ci_document_id_weight = 3\r\n",
        "Ci_receiver_id_weight = 2\r\n",
        "Ci_total_discount_weight = 1\r\n",
        "\r\n",
        "#Cross Reference Index weights\r\n",
        "xRefi_issuer_id_no_taxpayer_id_weight = 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import datetime\n",
        "import csv\n",
        "import pandas as pd\n",
        "from datetime import date, datetime\n",
        "from calendar import monthrange\n",
        "import time\n",
        "import pyodbc\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.functions import col, year, month, dayofmonth, isnan, when, count\n",
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, FloatType, DateType"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Initiate logging\n",
        "import logging\n",
        "from opencensus.ext.azure.log_exporter import AzureLogHandler\n",
        "from opencensus.ext.azure.trace_exporter import AzureExporter\n",
        "from opencensus.trace import config_integration\n",
        "from opencensus.trace.samplers import AlwaysOnSampler\n",
        "from opencensus.trace.tracer import Tracer\n",
        "\n",
        "instrumentation_connection_string = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"AppInsightsConnectionString\")\n",
        "config_integration.trace_integrations(['logging'])\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.addHandler(AzureLogHandler(connection_string=instrumentation_connection_string))\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "tracer = Tracer(\n",
        "    exporter=AzureExporter(\n",
        "        connection_string=instrumentation_connection_string\n",
        "    ),\n",
        "    sampler=AlwaysOnSampler()\n",
        ")\n",
        "\n",
        "# Spool parameters\n",
        "run_time_parameters = {'custom_dimensions': {\n",
        "    'batch_id': batch_id,\n",
        "    'statistics_path': statistics_path,\n",
        "    'data_encoding': data_encoding,\n",
        "    'data_separator': data_separator,\n",
        "    'invoice_schema_applied_path': invoice_schema_applied_path,\n",
        "    'taxpayer_profile_schema_applied_path': taxpayer_profile_schema_applied_path,\n",
        "    'notebook_name': mssparkutils.runtime.context['notebookname']\n",
        "} }\n",
        "  \n",
        "logger.info(f\"{mssparkutils.runtime.context['notebookname']}: INITIALISED\", extra=run_time_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# for every field it is needed to calculate the number of NULL records by month\n",
        "all_files_data_quality_schema = StructType([ \n",
        "    StructField(\"filename\",StringType(),True), \n",
        "    StructField(\"min_issued_date\",DateType(),True),\n",
        "    StructField(\"max_issued_date\",DateType(),True),\n",
        "    StructField(\"num_recs\",DoubleType(),True),\n",
        "    StructField(\"num_null_issuer_type\",DoubleType(),True),\n",
        "    StructField(\"num_null_issuer_id\",DoubleType(),True),\n",
        "    StructField(\"num_null_activity_issuer\",DoubleType(),True), \n",
        "    StructField(\"num_null_receiver_type\",DoubleType(),True),\n",
        "    StructField(\"num_null_receiver_id\",DoubleType(),True), \n",
        "    StructField(\"num_null_document_type\",DoubleType(),True),\n",
        "    StructField(\"num_null_document_id\",DoubleType(),True),  \n",
        "    StructField(\"num_null_issued_date\",DoubleType(),True),\n",
        "    StructField(\"num_null_sales_terms\",DoubleType(),True),\n",
        "    StructField(\"num_null_credit_term\",DoubleType(),True),\n",
        "    StructField(\"num_null_currency\",DoubleType(),True),\n",
        "    StructField(\"num_null_exchange_rate_r\",DoubleType(),True),\n",
        "    StructField(\"num_null_payment_method1\",DoubleType(),True),\n",
        "    StructField(\"num_null_payment_method2\",DoubleType(),True),\n",
        "    StructField(\"num_null_payment_method3\",DoubleType(),True),\n",
        "    StructField(\"num_null_payment_method4\",DoubleType(),True),\n",
        "    StructField(\"num_null_payment_method5\",DoubleType(),True),\n",
        "    StructField(\"num_null_payment_method99\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_taxable_services\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_non_taxable_service\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_taxable_goods\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_non_taxable_goods\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_taxable\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_non_taxable\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_sales\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_discounts\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_voucher\",DoubleType(),True),\n",
        "    StructField(\"num_null_total_tax\",DoubleType(),True),\n",
        "    StructField(\"num_invoices\",DoubleType(),True),\n",
        "    StructField(\"num_deb_notes\",DoubleType(),True),\n",
        "    StructField(\"num_credit_notes\",DoubleType(),True),\n",
        "    StructField(\"num_purchase_docs\",DoubleType(),True),\n",
        "    StructField(\"num_export_docs\",DoubleType(),True),\n",
        "    StructField(\"num_sellers\",DoubleType(),True),\n",
        "    StructField(\"num_buyers\",DoubleType(),True),\n",
        "    StructField(\"number_isics\",DoubleType(),True),\n",
        "    StructField(\"num_issuer_id_no_taxpayer_id\",DoubleType(),True)\n",
        "])\n",
        "\n",
        "unique_issuer_schema = StructType([StructField(\"issuer_id\",StringType(),True)])\n",
        "unique_receiver_schema = StructType([StructField(\"receiver_id\",StringType(),True)])\n",
        "unique_activity_issuer_schema = StructType([StructField(\"activity_issuer\",StringType(),True)])\n",
        "\n",
        "emptyRDD = spark.sparkContext.emptyRDD()\n",
        "df_all_files_data_quality = spark.createDataFrame(emptyRDD, schema=all_files_data_quality_schema)\n",
        "df_unique_issuer_id_dataset = spark.createDataFrame(emptyRDD, schema=unique_issuer_schema)\n",
        "df_unique_receiver_id_dataset = spark.createDataFrame(emptyRDD, schema=unique_receiver_schema)\n",
        "df_unique_activity_issuer_dataset = spark.createDataFrame(emptyRDD, schema=unique_activity_issuer_schema)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_total_nulls(df, colname) :\n",
        "    if [dtype for name, dtype in df.dtypes if name == colname][0] not in (\"timestamp\", \"date\"):\n",
        "       total_rows = df.where(col(colname).contains('None') | col(colname).contains('NULL') | (col(colname) == '' ) | col(colname).isNull() | isnan(colname)).count()\n",
        "    else:\n",
        "        # isnan isn't supported on timestamp and date types\n",
        "        total_rows = df.where(col(colname).contains('None') | col(colname).contains('NULL') | (col(colname) == '' ) | col(colname).isNull()).count()\n",
        "    return total_rows\n",
        "\n",
        "def qualify_totals(total_value):\n",
        "    if len(total_value) > 0 :\n",
        "        total_num = total_value[0][0]\n",
        "    else :\n",
        "        total_num = 0\n",
        "    return total_num"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_data_quality_of_file(df, fileName, df_taxpayer_profile) :\n",
        "    global df_unique_issuer_id_dataset\n",
        "    global df_unique_receiver_id_dataset\n",
        "    global df_unique_activity_issuer_dataset\n",
        "\n",
        "    df_counts = df.groupBy('document_type').count()\n",
        "\n",
        "    #invoices\n",
        "    total_number_sale_invoices = qualify_totals(df_counts.filter(df_counts.document_type == 'I').select('count').collect())\n",
        "    #debit notes\n",
        "    total_number_debit_notes = qualify_totals(df_counts.filter(df_counts.document_type == 'D').select('count').collect())\n",
        "    #credit notes\n",
        "    total_number_credit_notes = qualify_totals(df_counts.filter(df_counts.document_type == 'C').select('count').collect())\n",
        "    #purchase\n",
        "    total_number_purchase = qualify_totals(df_counts.filter(df_counts.document_type == 'P').select('count').collect())\n",
        "    #export invoice\n",
        "    total_number_export = qualify_totals(df_counts.filter(df_counts.document_type == 'X').select('count').collect())\n",
        "\n",
        "    total_issuer_ids = df.select('issuer_id').distinct().count()\n",
        "    total_receiver_ids = df.select('receiver_id').distinct().count()\n",
        "    total_ISICs = df.select('activity_issuer').distinct().count()\n",
        "    total_rows_in_dataset = df.count()\n",
        "\n",
        "    total_rows_null_issuer_id = get_total_nulls(df,\"issuer_id\") \n",
        "    total_rows_null_issued_date = get_total_nulls(df,\"issued_date\")\n",
        "    total_rows_null_issuer_type = get_total_nulls(df,\"issuer_type\")\n",
        "    total_rows_null_document_type = get_total_nulls(df,\"document_type\")\n",
        "    total_rows_null_currency = get_total_nulls(df,\"currency\")\n",
        "    total_rows_null_total_voucher = get_total_nulls(df,\"total_voucher\")\n",
        "    total_rows_null_total_tax = get_total_nulls(df,\"total_tax\")\n",
        "    total_rows_null_activity_issuer = get_total_nulls(df,\"activity_issuer\")\n",
        "    total_rows_null_document_id = get_total_nulls(df,\"document_id\")\n",
        "    total_rows_null_receiver_id = get_total_nulls(df,\"receiver_id\")\n",
        "    total_rows_null_total_taxable = get_total_nulls(df,\"total_taxable\")\n",
        "    total_rows_null_total_non_taxable = get_total_nulls(df,\"total_non_taxable\")\n",
        "    total_rows_null_total_sales = get_total_nulls(df,\"total_sales\")\n",
        "    total_rows_null_total_discount = get_total_nulls(df,\"total_discounts\")\n",
        "    total_rows_null_receiver_type = get_total_nulls(df,\"receiver_type\")\n",
        "    total_rows_null_sales_terms = get_total_nulls(df,\"sales_terms\")\n",
        "    total_rows_null_credit_terms = get_total_nulls(df,\"credit_term\")\n",
        "    total_rows_null_exchange_rate = get_total_nulls(df,\"exchange_rate_r\")\n",
        "    total_rows_null_payment_method1 = get_total_nulls(df,\"payment_method1\")\n",
        "    total_rows_null_payment_method2 = get_total_nulls(df,\"payment_method2\")\n",
        "    total_rows_null_payment_method3 = get_total_nulls(df,\"payment_method3\")\n",
        "    total_rows_null_payment_method4 = get_total_nulls(df,\"payment_method4\")\n",
        "    total_rows_null_payment_method5 = get_total_nulls(df,\"payment_method5\")\n",
        "    total_rows_null_payment_method99 = get_total_nulls(df,\"payment_method99\")\n",
        "    total_rows_null_total_taxable_services = get_total_nulls(df,\"total_taxable_services\")\n",
        "    total_rows_null_total_non_taxable_service = get_total_nulls(df,\"total_non_taxable_services\")\n",
        "    total_rows_null_total_taxable_goods = get_total_nulls(df,\"total_taxable_goods\")\n",
        "    total_rows_null_total_non_taxable_goods = get_total_nulls(df,\"total_non_taxable_goods\")\n",
        "\n",
        "    issued_date_min = df.agg({\"issued_date\": \"min\"}).collect()[0][0]\n",
        "    issued_date_max = df.agg({\"issued_date\": \"max\"}).collect()[0][0]\n",
        "        \n",
        "    df_unique_issuer_id_in_file = df.select('issuer_id').distinct()\n",
        "    \n",
        "    df_cross_reference = df_unique_issuer_id_in_file.join(df_taxpayer_profile, (df_unique_issuer_id_in_file['issuer_id'] == df_taxpayer_profile['taxpayer_id']) , how='left') \n",
        "    num_issuer_id_no_taxpayer_id = df_cross_reference.filter(col(\"taxpayer_id\").isNull()).count()\n",
        "    \n",
        "    new_row = [[fileName, issued_date_min, issued_date_max, \n",
        "                total_rows_in_dataset, total_rows_null_issuer_type, total_rows_null_issuer_id, total_rows_null_activity_issuer,\n",
        "                total_rows_null_receiver_type, total_rows_null_receiver_id, total_rows_null_document_type,\n",
        "                total_rows_null_document_id, total_rows_null_issued_date, total_rows_null_sales_terms, total_rows_null_credit_terms,\n",
        "                total_rows_null_currency, total_rows_null_exchange_rate, total_rows_null_payment_method1, total_rows_null_payment_method2,\n",
        "                total_rows_null_payment_method3, total_rows_null_payment_method4, total_rows_null_payment_method5,\n",
        "                total_rows_null_payment_method99, total_rows_null_total_taxable_services, total_rows_null_total_non_taxable_service,\n",
        "                total_rows_null_total_taxable_goods, total_rows_null_total_non_taxable_goods, total_rows_null_total_taxable,\n",
        "                total_rows_null_total_non_taxable, total_rows_null_total_sales, total_rows_null_total_discount, \n",
        "                total_rows_null_total_voucher, total_rows_null_total_tax, \n",
        "                total_number_sale_invoices, total_number_debit_notes, total_number_credit_notes, total_number_purchase, \n",
        "                total_number_export, total_issuer_ids, total_receiver_ids, total_ISICs, num_issuer_id_no_taxpayer_id\n",
        "                ]]\n",
        "    \n",
        "    df_unique_issuer_id_dataset = df_unique_issuer_id_dataset.union(df_unique_issuer_id_in_file)\n",
        "    df_unique_receiver_id_dataset = df_unique_receiver_id_dataset.union(df.select('receiver_id').distinct())\n",
        "    df_unique_activity_issuer_dataset = df_unique_activity_issuer_dataset.union(df.select('activity_issuer').distinct())\n",
        "\n",
        "    return new_row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Calculate the data quality of each file provided on a per-file basis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Reading taxpayer profile from ADLS'):\n",
        "    df_taxpayer_profile = spark.read.csv(f'{taxpayer_profile_schema_applied_path}/tax_payer_profile/', sep=data_separator,inferSchema=True, header=True).select('taxpayer_id')\n",
        "\n",
        "file_names = mssparkutils.fs.ls(invoice_schema_applied_path)\n",
        "\n",
        "firstFile = True\n",
        "for filename in file_names:\n",
        "    logger.info(f'Caclulating data quality for file: {filename.name}')\n",
        "    with tracer.span('Reading invoice file from ADLS'):\n",
        "         df = spark.read.csv(filename.path, sep=data_separator,inferSchema=True, header=True)\n",
        "\n",
        "    with tracer.span('Calculating data quality for invoice file'):\n",
        "        file_data_quality_row = get_data_quality_of_file(df, filename.name, df_taxpayer_profile)\n",
        "        df_all_files_data_quality = df_all_files_data_quality.union(spark.createDataFrame(file_data_quality_row))\n",
        "    \n",
        "    if firstFile == True:\n",
        "        df_alldata = df\n",
        "        firstFile = False\n",
        "    else:\n",
        "        df_alldata = df_alldata.union(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Calculate the aggregate data quality for the entire dataset across all files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Calcuate the in and out of range metrics for entire dataset'):\r\n",
        "    max_date = datetime.strptime(data_quality_max_date, \"%d-%m-%Y\")\r\n",
        "    min_date = datetime.strptime(data_quality_min_date, \"%d-%m-%Y\")\r\n",
        "\r\n",
        "    total_records = df_all_files_data_quality.select(F.sum('num_recs')).take(1)[0][0]\r\n",
        "    total_records_in_period_of_analysis = df_alldata.filter(df_alldata.issued_date >= min_date).filter(df_alldata.issued_date <= max_date).count()\r\n",
        "    total_records_out_of_period_of_analysis = total_records - total_records_in_period_of_analysis\r\n",
        "\r\n",
        "    total_records_below_range = df_alldata.filter(df_alldata.issued_date < min_date).count()\r\n",
        "    total_records_above_range = df_alldata.filter(df_alldata.issued_date > max_date).count()\r\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Calcuating aggregate data quality for entire dataset'):\r\n",
        "    df_dataset_data_quality = df_all_files_data_quality.select(\r\n",
        "                                            F.lit(min_date).alias('min_date'),\r\n",
        "                                            F.lit(max_date).alias('max_date'),\r\n",
        "                                            F.min('min_issued_date').alias('min_issued_date'),\r\n",
        "                                            F.max('max_issued_date').alias('max_issued_date'),\r\n",
        "                                            F.lit(total_records).alias('num_recs'),\r\n",
        "                                            F.lit(total_records_in_period_of_analysis).alias('num_recs_in_period_of_analysis'),\r\n",
        "                                            F.lit(total_records_out_of_period_of_analysis).alias('num_recs_out_period_of_analysis'),\r\n",
        "                                            F.sum('num_null_issuer_type').alias('num_null_issuer_type'),\r\n",
        "                                            F.sum('num_null_issuer_id').alias('num_null_issuer_id'),\r\n",
        "                                            F.sum('num_null_activity_issuer').alias('num_null_activity_issuer'),\r\n",
        "                                            F.sum('num_null_receiver_type').alias('num_null_receiver_type'),\r\n",
        "                                            F.sum('num_null_receiver_id').alias('num_null_receiver_id'),\r\n",
        "                                            F.sum('num_null_document_type').alias('num_null_document_type'),\r\n",
        "                                            F.sum('num_null_document_id').alias('num_null_document_id'),\r\n",
        "                                            F.sum('num_null_issued_date').alias('num_null_issued_date'),\r\n",
        "                                            F.sum('num_null_sales_terms').alias('num_null_sales_terms'),\r\n",
        "                                            F.sum('num_null_credit_term').alias('num_null_credit_term'),\r\n",
        "                                            F.sum('num_null_currency').alias('num_null_currency'),\r\n",
        "                                            F.sum('num_null_exchange_rate_r').alias('num_null_exchange_rate_r'),\r\n",
        "                                            F.sum('num_null_payment_method1').alias('num_null_payment_method1'),\r\n",
        "                                            F.sum('num_null_payment_method2').alias('num_null_payment_method2'),\r\n",
        "                                            F.sum('num_null_payment_method3').alias('num_null_payment_method3'),\r\n",
        "                                            F.sum('num_null_payment_method4').alias('num_null_payment_method4'),\r\n",
        "                                            F.sum('num_null_payment_method5').alias('num_null_payment_method5'),\r\n",
        "                                            F.sum('num_null_payment_method99').alias('num_null_payment_method99'),\r\n",
        "                                            F.sum('num_null_total_taxable_services').alias('num_null_total_taxable_services'),\r\n",
        "                                            F.sum('num_null_total_non_taxable_service').alias('num_null_total_non_taxable_service'),\r\n",
        "                                            F.sum('num_null_total_taxable_goods').alias('num_null_total_taxable_goods'),\r\n",
        "                                            F.sum('num_null_total_non_taxable_goods').alias('num_null_total_non_taxable_goods'),\r\n",
        "                                            F.sum('num_null_total_taxable').alias('num_null_total_taxable'),\r\n",
        "                                            F.sum('num_null_total_non_taxable').alias('num_null_total_non_taxable'),\r\n",
        "                                            F.sum('num_null_total_sales').alias('num_null_total_sales'),\r\n",
        "                                            F.sum('num_null_total_discounts').alias('num_null_total_discounts'),\r\n",
        "                                            F.sum('num_null_total_voucher').alias('num_null_total_voucher'),\r\n",
        "                                            F.sum('num_null_total_tax').alias('num_null_total_tax'),\r\n",
        "                                            F.sum('num_invoices').alias('num_invoices'),\r\n",
        "                                            F.sum('num_deb_notes').alias('num_deb_notes'),\r\n",
        "                                            F.sum('num_credit_notes').alias('num_credit_notes'),\r\n",
        "                                            F.sum('num_purchase_docs').alias('num_purchase_docs'),\r\n",
        "                                            F.sum('num_export_docs').alias('num_export_docs'),\r\n",
        "                                            ((F.lit(total_records_in_period_of_analysis)/F.lit(total_records))*100).alias('pct_in_range'),\r\n",
        "                                            ((F.lit(total_records_below_range)/F.lit(total_records))*100).alias('pct_below_range'),\r\n",
        "                                            ((F.lit(total_records_above_range)/F.lit(total_records))*100).alias('pct_above_range'),\r\n",
        "                                            ((F.sum('num_issuer_id_no_taxpayer_id')/F.lit(df_unique_issuer_id_dataset.distinct().count()))*100).alias('pct_issuer_id_no_taxpayer_id'),\r\n",
        "                                            F.lit(df_unique_issuer_id_dataset.distinct().count()).alias('num_unique_issuer_id'),\r\n",
        "                                            F.lit(df_unique_receiver_id_dataset.distinct().count()).alias('num_unique_receiver_id'),\r\n",
        "                                            F.lit(df_unique_activity_issuer_dataset.distinct().count()).alias('num_unique_ISICS')\r\n",
        "                                            )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Calculate the weighted and scored Quality Indexes for the entire dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def get_scoring_by_percentage_range(percentage) :\r\n",
        "    scoring_completeness = 1\r\n",
        "\r\n",
        "    if percentage == 0:\r\n",
        "        scoring_completeness = 1\r\n",
        "    elif ((percentage > 0) and (percentage <= 0.01)):\r\n",
        "        scoring_completeness = 0.9\r\n",
        "    elif ((percentage > 0.01) and (percentage <= 0.02)):\r\n",
        "        scoring_completeness = 0.5\r\n",
        "    elif ((percentage > 0.02) and (percentage <= 0.03)):\r\n",
        "        scoring_completeness = 0.25\r\n",
        "    else:\r\n",
        "        scoring_completeness = 0\r\n",
        "\r\n",
        "    return scoring_completeness"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Calcuating quality indexes for entire dataset'):\r\n",
        "    ds_pct_below_range = df_dataset_data_quality.select(\"pct_below_range\").take(1)[0][0] / 100\r\n",
        "\r\n",
        "    #Calculation of the scoring for ds_pct_below_range\r\n",
        "    if ds_pct_below_range <= 0.01:\r\n",
        "        SQi_below_range_score = 1\r\n",
        "    elif ((ds_pct_below_range > 0.01) and (ds_pct_below_range < 0.02)):\r\n",
        "        SQi_below_range_score = 0.9\r\n",
        "    elif ((ds_pct_below_range > 0.02) and (ds_pct_below_range < 0.05)):\r\n",
        "        SQi_below_range_score = 0.5\r\n",
        "    elif ds_pct_below_range > 0.05:\r\n",
        "        SQi_below_range_score = 0.25\r\n",
        "    else:\r\n",
        "        SQi_below_range_score = 0\r\n",
        "\r\n",
        "    logger.info(f'SQi_below_range_score: {SQi_below_range_score}')\r\n",
        "\r\n",
        "    #Calculation of the scoring for ds_pct_above_range\r\n",
        "    ds_pct_above_range = df_dataset_data_quality.select(\"pct_above_range\").take(1)[0][0] / 100\r\n",
        "\r\n",
        "    if ds_pct_above_range <= 0.01:\r\n",
        "        SQi_above_range_score = 1\r\n",
        "    elif ((ds_pct_above_range > 0.01) and (ds_pct_above_range < 0.02)):\r\n",
        "        SQi_above_range_score = 0.9\r\n",
        "    elif ((ds_pct_above_range > 0.02) and (ds_pct_above_range < 0.05)):\r\n",
        "        SQi_above_range_score = 0.5\r\n",
        "    elif ds_pct_above_range > 0.05:\r\n",
        "        SQi_above_range_score = 0.25\r\n",
        "    else:\r\n",
        "        SQi_above_range_score = 0\r\n",
        "    logger.info(f'SQi_above_range_score: {SQi_above_range_score}')\r\n",
        "\r\n",
        "    #Calculation of the scoring for ds_pct_in_range\r\n",
        "    ds_pct_in_range = df_dataset_data_quality.select(\"pct_in_range\").take(1)[0][0] / 100\r\n",
        "\r\n",
        "    if ds_pct_in_range >= 0.991:\r\n",
        "        SQi_in_range_score = 1\r\n",
        "    elif ((ds_pct_in_range > 0.975) and (ds_pct_in_range < 0.991)):\r\n",
        "        SQi_in_range_score = 0.9\r\n",
        "    elif ((ds_pct_in_range > 0.935) and (ds_pct_in_range < 0.975)):\r\n",
        "        SQi_in_range_score = 0.5\r\n",
        "    elif ds_pct_in_range < 0.935:\r\n",
        "        SQi_in_range_score = 0.25\r\n",
        "    else:\r\n",
        "        SQi_in_range_score = 0\r\n",
        "\r\n",
        "    SQi_max_weight = SQi_below_range_weight + SQi_above_range_weight + SQi_in_range_weight\r\n",
        "    #SQi = Average (Sigma (Parameter wt * score) / Max possible score >> across all defined criteria\r\n",
        "    SQi = ((SQi_below_range_weight * SQi_below_range_score) +  (SQi_above_range_weight * SQi_above_range_score) + \\\r\n",
        "            (SQi_in_range_weight * SQi_in_range_score )) / SQi_max_weight\r\n",
        "\r\n",
        "    logger.info(f'SQi: {SQi}')\r\n",
        "\r\n",
        "    # Calculation of Completeness index\r\n",
        "    total_rows_in_dataset = df_dataset_data_quality.select(\"num_recs\").take(1)[0][0]\r\n",
        "\r\n",
        "    pct_null_issuer_id = df_dataset_data_quality.select(\"num_null_issuer_id\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_issued_date = df_dataset_data_quality.select(\"num_null_issued_date\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_issuer_type = df_dataset_data_quality.select(\"num_null_issuer_type\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_document_type = df_dataset_data_quality.select(\"num_null_document_type\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_currency = df_dataset_data_quality.select(\"num_null_currency\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_exchange_rate = df_dataset_data_quality.select(\"num_null_exchange_rate_r\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_total_taxable = df_dataset_data_quality.select(\"num_null_total_taxable\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_total_non_taxable = df_dataset_data_quality.select(\"num_null_total_non_taxable\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_total_sales = df_dataset_data_quality.select(\"num_null_total_sales\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_total_voucher = df_dataset_data_quality.select(\"num_null_total_voucher\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_total_tax = df_dataset_data_quality.select(\"num_null_total_tax\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_activity_issuer = df_dataset_data_quality.select(\"num_null_activity_issuer\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_document_id = df_dataset_data_quality.select(\"num_null_document_id\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_receiver_id = df_dataset_data_quality.select(\"num_null_receiver_id\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "    pct_null_total_discount = df_dataset_data_quality.select(\"num_null_total_discounts\").take(1)[0][0] / total_rows_in_dataset\r\n",
        "\r\n",
        "\r\n",
        "    computed_score_issuer_id = get_scoring_by_percentage_range(pct_null_issuer_id)\r\n",
        "    computed_score_issued_date =  get_scoring_by_percentage_range(pct_null_issued_date)\r\n",
        "    computed_score_issuer_type =  get_scoring_by_percentage_range(pct_null_issuer_type)\r\n",
        "    computed_score_document_type =  get_scoring_by_percentage_range(pct_null_document_type)\r\n",
        "    computed_score_currency =  get_scoring_by_percentage_range(pct_null_currency)\r\n",
        "    computed_score_exchange_rate =  get_scoring_by_percentage_range(pct_null_exchange_rate)\r\n",
        "    computed_score_total_taxable =  get_scoring_by_percentage_range(pct_null_total_taxable)\r\n",
        "    computed_score_total_non_taxable = get_scoring_by_percentage_range(pct_null_total_non_taxable)\r\n",
        "    computed_score_total_sales =  get_scoring_by_percentage_range(pct_null_total_sales)\r\n",
        "    computed_score_total_voucher =  get_scoring_by_percentage_range(pct_null_total_voucher)\r\n",
        "    computed_score_total_tax =  get_scoring_by_percentage_range(pct_null_total_tax)\r\n",
        "    computed_score_activity_issuer =  get_scoring_by_percentage_range(pct_null_activity_issuer)\r\n",
        "    computed_score_document_id =  get_scoring_by_percentage_range(pct_null_document_id)\r\n",
        "    computed_score_receiver_id =  get_scoring_by_percentage_range(pct_null_receiver_id)\r\n",
        "    computed_score_total_discount =  get_scoring_by_percentage_range(pct_null_total_discount)\r\n",
        "\r\n",
        "    weighted_score_issuer_id = computed_score_issuer_id * Ci_issuer_id_weight\r\n",
        "    weighted_score_issued_date =  computed_score_issued_date * Ci_issued_date_weight\r\n",
        "    weighted_score_issuer_type =  computed_score_issuer_type * Ci_issuer_type_weight\r\n",
        "    weighted_score_document_type =  computed_score_document_type * Ci_document_type_weight\r\n",
        "    weighted_score_currency =  computed_score_currency * Ci_currency_weight\r\n",
        "    weighted_score_exchange_rate =  computed_score_exchange_rate * Ci_exchange_rate_weight\r\n",
        "    weighted_score_total_taxable =  computed_score_total_taxable * Ci_total_taxable_weight\r\n",
        "    weighted_score_total_non_taxable = computed_score_total_non_taxable * Ci_total_non_taxable_weight\r\n",
        "    weighted_score_total_sales =  computed_score_total_sales * Ci_total_sales_weight\r\n",
        "    weighted_score_total_voucher =  computed_score_total_voucher * Ci_total_voucher_weight\r\n",
        "    weighted_score_total_tax =  computed_score_total_tax * Ci_total_tax_weight\r\n",
        "    weighted_score_activity_issuer =  computed_score_activity_issuer * Ci_activity_issuer_weight\r\n",
        "    weighted_score_document_id =  computed_score_document_id * Ci_document_id_weight\r\n",
        "    weighted_score_receiver_id =  computed_score_receiver_id * Ci_receiver_id_weight\r\n",
        "    weighted_score_total_discounts =  computed_score_total_discount * Ci_total_discount_weight\r\n",
        "\r\n",
        "    weighted_score_total = weighted_score_issuer_id + weighted_score_issued_date + weighted_score_issuer_type + weighted_score_document_type + \\\r\n",
        "                        weighted_score_currency + weighted_score_exchange_rate + weighted_score_total_taxable + weighted_score_total_non_taxable + \\\r\n",
        "                        weighted_score_total_sales + weighted_score_total_voucher + weighted_score_total_tax + weighted_score_activity_issuer + \\\r\n",
        "                        weighted_score_document_id + weighted_score_receiver_id + weighted_score_total_discounts\r\n",
        "\r\n",
        "    Ci_max_weight = Ci_issuer_id_weight + Ci_issued_date_weight + Ci_issuer_type_weight + Ci_document_type_weight + \\\r\n",
        "                            Ci_currency_weight + Ci_exchange_rate_weight + Ci_total_taxable_weight + Ci_total_non_taxable_weight + \\\r\n",
        "                            Ci_total_sales_weight + Ci_total_voucher_weight + Ci_total_tax_weight + Ci_activity_issuer_weight + \\\r\n",
        "                            Ci_document_id_weight + Ci_receiver_id_weight + Ci_total_discount_weight \r\n",
        "\r\n",
        "    #Ci = Average (Sigma (Parameter wt * score) / Max possible score >> across all defined criteria parameters.\r\n",
        "    Ci = weighted_score_total / Ci_max_weight\r\n",
        "    logger.info(f'Ci: {Ci}')\r\n",
        "\r\n",
        "    # Cross Reference index\r\n",
        "    ds_pct_issuer_id_no_taxpayer_id = df_dataset_data_quality.select(\"pct_issuer_id_no_taxpayer_id\").take(1)[0][0] / 100\r\n",
        "\r\n",
        "    computed_score_issuer_id_no_taxpayer_id = get_scoring_by_percentage_range(ds_pct_issuer_id_no_taxpayer_id)\r\n",
        "    weighted_score_issuer_id_no_taxpayer_id = xRefi_issuer_id_no_taxpayer_id_weight * computed_score_issuer_id_no_taxpayer_id\r\n",
        "\r\n",
        "    #xRefi = Average (Sigma (Parameter wt * score) / Max possible score >> across all defined cross reference data sources\r\n",
        "    xRefi = weighted_score_issuer_id_no_taxpayer_id / xRefi_issuer_id_no_taxpayer_id_weight\r\n",
        "    logger.info(f'xRefi: {xRefi}')\r\n",
        "\r\n",
        "    #Overall Data Quality Index (DQi) is defined as the average of all the three scores : Average (SQi, Ci, xRefi)\r\n",
        "    DQi = (SQi + Ci + xRefi) / 3\r\n",
        "    logger.info(f'DQi: {DQi}')\r\n",
        "\r\n",
        "    df_dataset_data_quality = df_dataset_data_quality.withColumn('SQi', F.lit(SQi)).withColumn('Ci', F.lit(Ci)).withColumn('xRefi', F.lit(xRefi)).withColumn('DQi', F.lit(DQi))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Write out the statistics\n",
        "if the notebook receives the \"run_time_stamp\" parameter from Synapse Ingest the parameter is used to create a folder to store the statistics, if no parameter is received we write the statistics to the default output folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Saving data quality statistics to ADLS'):\n",
        "    #writeout statistics\n",
        "    df_all_files_data_quality.repartition(1).write.mode(\"overwrite\").parquet(f'{statistics_path}file_data_quality')\n",
        "    df_dataset_data_quality.repartition(1).write.mode(\"overwrite\").parquet(f'{statistics_path}dataset_data_quality')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Write out SQL table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# serverless SQL config\n",
        "database = 'eiad'\n",
        "driver= '{ODBC Driver 17 for SQL Server}'\n",
        "\n",
        "sql_user_name = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLUserName\")\n",
        "sql_user_pwd = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SynapseSQLPassword\")\n",
        "serverless_sql_endpoint = mssparkutils.credentials.getSecretWithLS(\"keyvault\", \"SyanpseServerlessSQLEndpoint\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def generate_schema_string(dataframe):\n",
        "    schema_string = \"\"\n",
        "    for name in dataframe.schema.fieldNames():\n",
        "        schema_string += \"[\" + name + \"] \"\n",
        "        datatype = str(dataframe.schema[name].dataType.simpleString())\n",
        "        if datatype == 'double': datatype = 'float'\n",
        "        if datatype == 'string': datatype = 'nvarchar(MAX)'\n",
        "        if datatype == 'timestamp': datatype = 'datetime2(7)'\n",
        "        schema_string += datatype + \", \"\n",
        "    return schema_string[:-2]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for per file data quality'):\n",
        "    table_name = statistics_path.split('/')[3] + '_' + statistics_path.split('/')[2].split('@')[0] + '_' + statistics_path.split('/')[4] + '_' + 'file_data_quality'\n",
        "    schema_string = generate_schema_string(df_all_files_data_quality)\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "    location = \"/\".join([i for idx, i in enumerate(statistics_path.split('/')) if idx > 2]) + 'file_data_quality'\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            try:\n",
        "                cursor.execute(drop_table_command)\n",
        "            except:\n",
        "                pass\n",
        "            cursor.execute(df_sql_command)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "with tracer.span('Creating SQL table for full dataset data quality'):\n",
        "    table_name = statistics_path.split('/')[3] + '_' + statistics_path.split('/')[2].split('@')[0] + '_' + statistics_path.split('/')[4] + '_' + 'dataset_data_quality'\n",
        "    schema_string = generate_schema_string(df_dataset_data_quality)\n",
        "    drop_table_command = f\"DROP EXTERNAL TABLE [{table_name}]\"\n",
        "    location = \"/\".join([i for idx, i in enumerate(statistics_path.split('/')) if idx > 2]) + 'dataset_data_quality'\n",
        "    df_sql_command = f\"CREATE EXTERNAL TABLE [{table_name}] ({schema_string}) WITH (LOCATION = '{location}/**', DATA_SOURCE = [output_<<STORAGE_ACCOUNT_NAME>>_dfs_core_windows_net], FILE_FORMAT = [SynapseParquetFormat])\"\n",
        "    with pyodbc.connect('DRIVER='+driver+';SERVER=tcp:'+serverless_sql_endpoint+';PORT=1433;DATABASE='+database+';UID='+sql_user_name+';PWD='+ sql_user_pwd) as conn:\n",
        "        with conn.cursor() as cursor:\n",
        "            try:\n",
        "                cursor.execute(drop_table_command)\n",
        "            except:\n",
        "                pass\n",
        "            cursor.execute(df_sql_command)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
